[
["index.html", "Math 116: Elementary Statistic Using R and R Studio Chapter 1 Intruduction 1.1 Load these packages before doing anyting 1.2 Need to use the following dataset 1.3 View the dataset 1.4 Another way to view the dataset, simply type the name of the dataset 1.5 The first 5 observations of a dataset 1.6 The last 5 observations of a dataset 1.7 Random 5 observations 1.8 Structure of a dataset 1.9 Discriptive Statistics 1.10 Describing the Center: The Mean 1.11 Population mean 1.12 A small example: 1.13 Weighted Mean 1.14 Describing the Spread: the SD 1.15 Describing the Center: The Median 1.16 Quantiles 1.17 Describing Shapes 1.18 Boxplots 1.19 Another way 1.20 Boxplot Detect Outliers 1.21 Compare distributions 1.22 z-scores 1.23 Let’s Practice.", " Math 116: Elementary Statistic Using R and R Studio Rong You 2019-08-26 Chapter 1 Intruduction These were my lecture notes used when I taught Math 115 using R and R Studio. Many examples were taken from Elementary Statistics by Mario F. Triola., Prof. Homer White’s website from Geogetown College, and my AP Statistics lecture notes. 1.1 Load these packages before doing anyting require(mosaic) require(tigerstats) require(manipulate) 1.2 Need to use the following dataset data(m111survey) &lt;!-- Shortcut for commenting &quot;Ctrl+Shift+C&quot; for PC &quot;Command+Shift+C&quot; for Mac --&gt; 1.3 View the dataset View(m111survey) 1.4 Another way to view the dataset, simply type the name of the dataset shortcut of inserting R code chucks: Ctrl-Alt-I on PC and Cmd-Option-I on Mac) m111survey ## height ideal_ht sleep fastest weight_feel love_first extra_life ## 1 76.00 78.0 9.5 119 1_underweight no yes ## 2 74.00 76.0 7.0 110 2_about_right no yes ## 3 64.00 NA 9.0 85 2_about_right no no ## 4 62.00 65.0 7.0 100 1_underweight no no ## 5 72.00 72.0 8.0 95 1_underweight no yes ## 6 70.80 NA 10.0 100 3_overweight no no ## 7 70.00 72.0 4.0 85 2_about_right no yes ## 8 79.00 76.0 6.0 160 2_about_right no yes ## 9 59.00 61.0 7.0 90 2_about_right no yes ## 10 67.00 67.0 7.0 90 3_overweight no no ## 11 65.00 69.0 6.0 100 2_about_right no no ## 12 62.00 62.0 7.0 60 3_overweight no no ## 13 59.00 62.0 5.0 80 2_about_right yes yes ## 14 78.00 75.0 7.0 80 3_overweight no no ## 15 69.00 72.0 7.0 125 1_underweight no no ## 16 68.00 68.0 4.5 100 1_underweight yes yes ## 17 73.00 77.0 6.0 110 2_about_right yes yes ## 18 73.00 75.0 8.0 120 2_about_right no yes ## 19 65.00 70.0 8.0 120 3_overweight yes yes ## 20 65.00 68.0 7.0 125 3_overweight yes no ## 21 66.00 68.0 7.0 75 3_overweight no no ## 22 67.75 70.0 7.0 90 3_overweight no no ## 23 63.00 67.0 8.5 90 3_overweight no yes ## 24 66.00 66.0 7.0 120 3_overweight yes no ## 25 68.00 68.0 4.0 90 3_overweight yes no ## 26 54.00 54.0 4.0 130 3_overweight yes yes ## 27 74.00 75.0 5.0 119 2_about_right yes yes ## 28 68.00 66.0 4.5 112 2_about_right yes no ## 29 68.00 68.0 6.0 93 2_about_right yes yes ## 30 69.00 67.0 6.0 145 3_overweight no no ## 31 72.00 90.0 9.0 125 3_overweight no yes ## 32 70.50 73.0 7.0 190 2_about_right no yes ## 33 70.00 75.0 7.5 90 2_about_right yes yes ## 34 75.00 78.0 7.0 143 3_overweight yes no ## 35 72.00 75.0 7.0 120 2_about_right yes no ## 36 62.00 62.0 4.5 95 3_overweight no no ## 37 74.00 76.0 5.0 115 3_overweight yes no ## 38 63.00 67.0 7.5 105 3_overweight yes no ## 39 69.00 66.0 6.0 100 3_overweight no no ## 40 60.00 66.0 5.0 95 3_overweight yes no ## 41 68.00 69.5 9.0 95 3_overweight no no ## 42 73.00 76.0 8.0 110 3_overweight no yes ## 43 66.00 68.0 9.0 91 3_overweight yes no ## 44 70.00 67.0 5.5 85 3_overweight yes no ## 45 51.00 54.0 7.0 130 2_about_right no no ## 46 67.00 68.0 7.0 104 3_overweight yes no ## 47 69.00 70.0 5.0 95 3_overweight no no ## 48 71.00 67.0 3.0 105 3_overweight no no ## 49 74.00 74.0 5.0 90 3_overweight no no ## 50 65.00 67.0 6.5 90 2_about_right no yes ## 51 63.50 65.0 6.5 105 2_about_right no no ## 52 66.00 68.0 4.5 95 3_overweight yes yes ## 53 69.00 65.0 8.0 110 3_overweight yes yes ## 54 75.00 77.0 7.0 105 3_overweight no no ## 55 65.00 75.0 6.0 130 2_about_right no yes ## 56 74.00 76.0 7.0 95 2_about_right no yes ## 57 64.00 66.0 6.0 95 3_overweight yes no ## 58 76.00 77.0 7.0 100 2_about_right no yes ## 59 64.00 68.0 6.0 110 3_overweight no yes ## 60 71.50 74.0 6.0 108 2_about_right no no ## 61 63.00 68.0 7.5 75 3_overweight no no ## 62 64.00 68.0 7.5 102 3_overweight no no ## 63 68.00 72.0 6.5 105 3_overweight no yes ## 64 70.00 72.0 4.0 98 1_underweight yes yes ## 65 68.00 72.0 4.0 135 1_underweight no yes ## 66 75.00 75.0 6.0 130 2_about_right no yes ## 67 69.00 67.0 2.0 85 3_overweight yes no ## 68 70.00 72.0 5.0 85 1_underweight no no ## 69 61.00 68.0 5.0 130 2_about_right no no ## 70 65.00 66.0 8.0 120 2_about_right yes no ## 71 70.00 73.0 5.0 110 1_underweight no no ## seat GPA enough_Sleep sex diff.ideal.act. ## 1 1_front 3.560 no male 2.00 ## 2 2_middle 2.500 no male 2.00 ## 3 2_middle 3.800 no female NA ## 4 1_front 3.500 no female 3.00 ## 5 3_back 3.200 no male 0.00 ## 6 1_front 3.100 yes male NA ## 7 1_front 3.680 no male 2.00 ## 8 3_back 2.700 yes male -3.00 ## 9 3_back 2.800 no female 2.00 ## 10 2_middle NA yes female 0.00 ## 11 1_front 2.100 yes female 4.00 ## 12 1_front 2.500 yes female 0.00 ## 13 1_front 3.890 no female 3.00 ## 14 2_middle 3.200 yes female -3.00 ## 15 2_middle 3.200 no male 3.00 ## 16 2_middle 2.200 no male 0.00 ## 17 3_back 3.500 no male 4.00 ## 18 2_middle 3.550 yes male 2.00 ## 19 1_front 3.750 yes female 5.00 ## 20 3_back 3.500 no female 3.00 ## 21 1_front 3.400 yes female 2.00 ## 22 1_front 2.770 no male 2.25 ## 23 1_front 3.000 yes female 4.00 ## 24 1_front 3.167 yes female 0.00 ## 25 2_middle 3.200 no female 0.00 ## 26 1_front 3.413 no female 0.00 ## 27 1_front 3.700 no male 1.00 ## 28 1_front 3.500 no female -2.00 ## 29 1_front 3.750 no female 0.00 ## 30 3_back 2.800 no female -2.00 ## 31 3_back 2.200 no male 18.00 ## 32 3_back 3.600 no male 2.50 ## 33 2_middle 2.800 yes male 5.00 ## 34 2_middle 3.100 yes male 3.00 ## 35 2_middle 3.000 no male 3.00 ## 36 2_middle 3.500 no female 0.00 ## 37 1_front 3.900 no male 2.00 ## 38 1_front 3.787 yes female 4.00 ## 39 2_middle 3.200 no female -3.00 ## 40 2_middle 3.700 no female 6.00 ## 41 2_middle 2.000 yes female 1.50 ## 42 2_middle 3.100 yes male 3.00 ## 43 1_front 3.500 no female 2.00 ## 44 2_middle 3.700 no female -3.00 ## 45 1_front 2.550 no female 3.00 ## 46 2_middle 3.730 no female 1.00 ## 47 2_middle 3.500 no female 1.00 ## 48 2_middle 3.200 no female -4.00 ## 49 2_middle 3.100 no male 0.00 ## 50 2_middle 3.900 yes female 2.00 ## 51 2_middle 3.250 no female 1.50 ## 52 2_middle 3.200 no female 2.00 ## 53 3_back 3.294 no female -4.00 ## 54 2_middle 2.000 yes male 2.00 ## 55 1_front 3.000 no male 10.00 ## 56 3_back 3.700 yes male 2.00 ## 57 1_front 3.300 no female 2.00 ## 58 2_middle 3.200 yes male 1.00 ## 59 2_middle 3.600 yes female 4.00 ## 60 2_middle 3.300 no male 2.50 ## 61 1_front 4.000 yes female 5.00 ## 62 1_front 3.400 no female 4.00 ## 63 2_middle 2.700 yes male 4.00 ## 64 3_back 2.900 no male 2.00 ## 65 2_middle 1.900 no male 4.00 ## 66 2_middle 2.800 yes male 0.00 ## 67 1_front 3.500 no female -2.00 ## 68 2_middle 3.300 no male 2.00 ## 69 1_front 3.700 no female 7.00 ## 70 3_back 2.914 yes female 1.00 ## 71 1_front 2.700 no male 3.00 1.5 The first 5 observations of a dataset head(m111survey) ## height ideal_ht sleep fastest weight_feel love_first extra_life ## 1 76.0 78 9.5 119 1_underweight no yes ## 2 74.0 76 7.0 110 2_about_right no yes ## 3 64.0 NA 9.0 85 2_about_right no no ## 4 62.0 65 7.0 100 1_underweight no no ## 5 72.0 72 8.0 95 1_underweight no yes ## 6 70.8 NA 10.0 100 3_overweight no no ## seat GPA enough_Sleep sex diff.ideal.act. ## 1 1_front 3.56 no male 2 ## 2 2_middle 2.50 no male 2 ## 3 2_middle 3.80 no female NA ## 4 1_front 3.50 no female 3 ## 5 3_back 3.20 no male 0 ## 6 1_front 3.10 yes male NA 1.6 The last 5 observations of a dataset tail(m111survey) ## height ideal_ht sleep fastest weight_feel love_first extra_life ## 66 75 75 6 130 2_about_right no yes ## 67 69 67 2 85 3_overweight yes no ## 68 70 72 5 85 1_underweight no no ## 69 61 68 5 130 2_about_right no no ## 70 65 66 8 120 2_about_right yes no ## 71 70 73 5 110 1_underweight no no ## seat GPA enough_Sleep sex diff.ideal.act. ## 66 2_middle 2.800 yes male 0 ## 67 1_front 3.500 no female -2 ## 68 2_middle 3.300 no male 2 ## 69 1_front 3.700 no female 7 ## 70 3_back 2.914 yes female 1 ## 71 1_front 2.700 no male 3 1.7 Random 5 observations m111survey[sample(nrow(m111survey), 5), ] ## height ideal_ht sleep fastest weight_feel love_first extra_life ## 40 60 66 5.0 95 3_overweight yes no ## 36 62 62 4.5 95 3_overweight no no ## 16 68 68 4.5 100 1_underweight yes yes ## 61 63 68 7.5 75 3_overweight no no ## 11 65 69 6.0 100 2_about_right no no ## seat GPA enough_Sleep sex diff.ideal.act. ## 40 2_middle 3.7 no female 6 ## 36 2_middle 3.5 no female 0 ## 16 2_middle 2.2 no male 0 ## 61 1_front 4.0 yes female 5 ## 11 1_front 2.1 yes female 4 1.8 Structure of a dataset str(m111survey) ## &#39;data.frame&#39;: 71 obs. of 12 variables: ## $ height : num 76 74 64 62 72 70.8 70 79 59 67 ... ## $ ideal_ht : num 78 76 NA 65 72 NA 72 76 61 67 ... ## $ sleep : num 9.5 7 9 7 8 10 4 6 7 7 ... ## $ fastest : int 119 110 85 100 95 100 85 160 90 90 ... ## $ weight_feel : Factor w/ 3 levels &quot;1_underweight&quot;,..: 1 2 2 1 1 3 2 2 2 3 ... ## $ love_first : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ extra_life : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 2 1 2 2 2 1 ... ## $ seat : Factor w/ 3 levels &quot;1_front&quot;,&quot;2_middle&quot;,..: 1 2 2 1 3 1 1 3 3 2 ... ## $ GPA : num 3.56 2.5 3.8 3.5 3.2 3.1 3.68 2.7 2.8 NA ... ## $ enough_Sleep : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 2 1 2 1 2 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 1 2 2 2 2 1 1 ... ## $ diff.ideal.act.: num 2 2 NA 3 0 NA 2 -3 2 0 ... 1.9 Discriptive Statistics 1.9.1 Tally the sexes (a table of counts): xtabs(~sex,data=m111survey) ## sex ## female male ## 40 31 1.9.2 Another way to get counts table(m111survey$sex) ## ## female male ## 40 31 1.9.3 Get percentages: rowPerc(xtabs(~sex,data=m111survey)) # case sensitive ## ## sex female male Total ## 56.34 43.66 100 1.9.4 Barcharts barchartGC(~sex,data=m111survey, type=&quot;percent&quot;, main=&quot;Distribution of Sex&quot;, xlab=&quot;Sex&quot;, ylab=&quot;Percentages&quot;) 1.9.5 contingency table xtabs(~sex+seat,data=m111survey) ## seat ## sex 1_front 2_middle 3_back ## female 19 16 5 ## male 8 16 7 1.9.6 Another way table(m111survey$sex, m111survey$seat) ## ## 1_front 2_middle 3_back ## female 19 16 5 ## male 8 16 7 1.9.7 Row Percents sexseat &lt;- xtabs(~sex+seat,data=m111survey) rowPerc(sexseat) ## seat ## sex 1_front 2_middle 3_back Total ## female 47.50 40.00 12.50 100.00 ## male 25.81 51.61 22.58 100.00 1.9.8 Barcharts to compare barchartGC(~sex+seat,data=m111survey, type=&quot;percent&quot;, main=&quot;Seating Preference by Sex&quot;) 1.9.9 Describing a Numerical Variable favstats(~fastest,data=m111survey) ## min Q1 median Q3 max mean sd n missing ## 60 90.5 102 119.5 190 105.9014 20.8773 71 0 1.9.10 or anther way summary(m111survey$fastest) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 60.0 90.5 102.0 105.9 119.5 190.0 1.10 Describing the Center: The Mean sample mean \\[\\bar{x}=\\frac{\\sum x_i}{n}\\] where n is the sample size 1.11 Population mean \\[\\mu=\\frac{\\sum x_i}{n}\\] 1.12 A small example: FakeData &lt;- c(2,4,7,9,10) mean(FakeData) # Find the mean ## [1] 6.4 1.13 Weighted Mean When data values are assigned different weights, \\(w\\), we can compute a weighted mean. \\[\\bar{x}=\\frac{\\sum{(w\\cdot x)}}{\\sum{w}}\\] In her first semester of college, a student of the author took five courses. Her final grades along with the number of credits for each course were A (3 credits), A (4 credits), B (3 credits), C (3 credits), and F (1 credit). The grading system assigns quality points to letter grades as follows: A = 4; B = 3; C = 2; D = 1; F = 0. Compute her grade point average. 1.14 Describing the Spread: the SD 1.14.1 Standard deviation (SD) measures how far the typical data value is from the mean of all the data. –&gt; 1.14.2 Sample Standard Deviation \\[s=\\sqrt{\\frac{\\sum (x_i-\\bar{x})^2}{n-1}}\\] 1.14.3 Population stardar deviation \\[\\sigma=\\sqrt{\\frac{\\sum (x_i-\\mu)^2}{n}}\\] 1.14.4 Don’t calculatie SD by Hand sd(m111survey$fastest) ## [1] 20.8773 1.15 Describing the Center: The Median median(FakeData) ## [1] 7 1.16 Quantiles 1.16.1 Also called percentiles. with(m111survey, quantile(fastest, probs=c(0.2,0.5,0.8,0.9))) # you can change any decimal if you want. i.e 0.4 means 40% percentile ## 20% 50% 80% 90% ## 90 102 120 130 1.16.2 Quartiles with(m111survey, quantile(fastest, probs=c(0.25,0.50,0.75))) ## 25% 50% 75% ## 90.5 102.0 119.5 1.16.3 Inverting a Quantile Given an observation x from your data, you want to know its corresponding quantile, that is, you want to know what fraction of the data is less than x mean(m111survey$fastest&lt;100) ## [1] 0.4084507 1.17 Describing Shapes 1.17.1 Histogram hist(m111survey$fastest) # Change the title and x, y descriptions 1.17.2 Relative frequency histogram 1.17.3 Making a Density Histogram histogram(~fastest, data=m111survey, xlab=&quot;speed (mph)&quot;, main=&quot;Fastest Speed&quot;) 1.17.4 Making a Density Plot densityplot(~fastest, data=m111survey, xlab=&quot;speed (mph)&quot;, main=&quot;Fastest Speed&quot;) 1.18 Boxplots ImaginaryData &lt;- c(7.1,7.3,7.5,8.2,8.5,9.1,9.5, 9.8,9,9,9.9,10,10.5,11) bwplot(~ImaginaryData,xlab=&quot;x&quot;, main=&quot;Example Boxplot&quot;) 1.19 Another way boxplot(ImaginaryData, horizontal = TRUE) 1.20 Boxplot Detect Outliers boxplot(m111survey$height, horizontal = TRUE) # Outliers: less than Q1-1.5IQR or greater than Q3+1.5IQR boxplot.stats(m111survey$height) #identify outliers ## $stats ## [1] 59.00 65.00 68.00 71.75 79.00 ## ## $n ## [1] 71 ## ## $conf ## [1] 66.7343 69.2657 ## ## $out ## [1] 54 51 1.21 Compare distributions favstats(fastest~sex,data=m111survey) ## sex min Q1 median Q3 max mean sd n missing ## 1 female 60 90 95 110.0 145 100.0500 17.60966 40 0 ## 2 male 85 99 110 122.5 190 113.4516 22.56818 31 0 bwplot(fastest~sex, data=m111survey, xlab=&quot;speed (mph)&quot;, main=&quot;Fastest Speed&quot;) histogram(~fastest|sex,data=m111survey, type=&quot;density&quot;, main=&quot;Fastest Speed Driven, by Sex&quot;, xlab=&quot;Fastest Speed, in mph&quot;) 1.22 z-scores \\[z=\\frac{x-\\bar{x}}{s}\\] Question: Suppose that Linda is 72 inches tall. How does she compare with the other GC students in the m111survey data? Solution: Get her z-score. favstats(~height,data=m111survey) ## min Q1 median Q3 max mean sd n missing ## 51 65 68 71.75 79 67.98662 5.296414 71 0 Question: Is Linda unusually tall, for a female? Which means does her height fall with 2 stander deviations about the mean? If yes, then her height is normal. If not, then her height is unusually tall (or short). favstats(height~sex,data=m111survey) ## sex min Q1 median Q3 max mean sd n missing ## 1 female 51 63 65 68 78 64.93750 4.621837 40 0 ## 2 male 65 70 72 74 79 71.92097 3.048545 31 0 1.23 Let’s Practice. R has some embedded datasets. Let’s use the dataset chickwts 1.23.1 Load the dataset View(chickwts) 1.23.2 You want to know what the dataset is about. ?chickwts ## starting httpd help server ... done 1.23.3 Data Structure str(chickwts) ## &#39;data.frame&#39;: 71 obs. of 2 variables: ## $ weight: num 179 160 136 227 217 168 108 124 143 140 ... ## $ feed : Factor w/ 6 levels &quot;casein&quot;,&quot;horsebean&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... 1.23.4 How many differnt types of feed? table(chickwts$feed) ## ## casein horsebean linseed meatmeal soybean sunflower ## 12 10 12 11 14 12 1.23.5 Percentages? rowPerc(xtabs(~feed,data=chickwts)) ## ## feed casein horsebean linseed meatmeal soybean sunflower Total ## 16.9 14.08 16.9 15.49 19.72 16.9 100 1.23.6 Get a bar charts for the numbers of different feeds barchartGC(~feed,data=chickwts, type=&quot;percent&quot;, main=&quot;Percent Comparison in Different Feeds &quot;, xlab=&quot;Types of Feeds&quot;) "],
["surveys-and-experiments.html", "Chapter 2 Surveys and Experiments 2.1 Statistics: 2.2 Population 2.3 Census 2.4 Sample 2.5 Example 2.6 Statistical Thinking Procedure: 2.7 Potential Pitfalls–Misleading Conclusions 2.8 Parameter 2.9 Statistic 2.10 Quantitative (or numerical) data 2.11 Categorical (or qualitative or attribute) data 2.12 Discrete data 2.13 Continuous (numerical) data 2.14 Example 2.15 Basics of Collecting Data 2.16 Observational Study: Survey 2.17 Identify the sampling design: 2.18 Experiment", " Chapter 2 Surveys and Experiments 2.1 Statistics: The science of planning studies and experiments, obtaining data, and then organizing, summarizing, presenting, analyzing, interpreting, and drawing conclusions based on the data. 2.2 Population The complete collection of all measurements or data that are being considered 2.3 Census Collection of data from every member of a population 2.4 Sample Subcollection of members selected from a population 2.5 Example The student senate at a university with 15,000 students is interested in the proportion of students who favor a change in the grading system to allow for plus and minus grades (e.g. B+, B, B-, rather than just B). Two hundred students are interviewed to determine their attitude toward this proposed change. What is the population of interest? What group of students constitutes the sample in this problem? Population:The student senate at a university with 15,000 students Sample: 200 students are interviewed. 2.6 Statistical Thinking Procedure: Prepare Analyze Conclude 2.7 Potential Pitfalls–Misleading Conclusions Concluding that one variable causes the other variable when in fact the variables are only correlated or associated together. Small Samples Nonresponse Missing Data Percentages 2.8 Parameter a numerical measurement describing some characteristic of a population. 2.9 Statistic a numerical measurement describing some characteristic of a sample. 2.10 Quantitative (or numerical) data consists of numbers representing counts or measurements. 2.11 Categorical (or qualitative or attribute) data consists of names or labels (representing categories). 2.12 Discrete data result when the number of possible values is either a finite number or a countable number 2.13 Continuous (numerical) data result from infinitely many possible values that correspond to some continuous scale that covers a range of values without gaps, interruptions, or jumps. 2.14 Example Identify the following variables: the appraised value of homes in Frisco the color of cars in the teacher’s lot the number of calculators owned by students at your school the zip code of an individual the amount of time it takes students to drive to school 2.15 Basics of Collecting Data Observational study observing and measuring specific characteristics without attempting to modify the subjects being studied. Experiment apply some treatment and then observe its effects on the subjects (subjects in experiments are called experimental units) 2.16 Observational Study: Survey Sample: A part of the population that we actually examine in order to gather information. We Use sample to generalize to population. Sampling Methods: Simple Random Sample (SRS): consist of n individuals from the population chosen in such a way that every individual has an equal chance of being selected and every set of n individuals has an equal chance of being selected. library(tigerstats) #View(FakeSchool) mu &lt;- mean(~GPA,data=FakeSchool) mu ## [1] 2.766429 The population mean, which is \\(\\mu=2.766\\) Let’s use SRS to take 7 samples: set.seed(314159) srs &lt;- popsamp(7,FakeSchool) srs ## Students Sex class GPA Honors ## 10 Chris M So 4.0 Yes ## 2 Brad M Fr 2.6 Yes ## 16 Brittany F Jr 3.9 No ## 20 Eliott M Jr 1.9 No ## 21 Garth M Jr 1.1 No ## 23 Bob M Sr 3.8 Yes ## 13 Eric M So 2.1 No xbar.srs &lt;- mean(~GPA,data=srs) xbar.srs ## [1] 2.771429 The sample mean using SRS, which is \\(\\bar{x}=2.525\\) Strengths The selection of one element does not affect the selection of others. Each possible sample, of a given size, has an equal chance of being selected. Simple random samples tend to be good representations of the population. Requires little knowledge of the population. Weaknesses If there are small subgroups within the population, a SRS may not give an accurate representation of that subgroup. In fact, it may not include it at all! This is especially true if the sample size is small. If the population is large and widely dispersed, it can be costly (both in time and money) to collect the data. Systematic Sampling In a systematic sample, the members of the population are put in a row. Then 1 out of every k members are selected. The starting point is randomly chosen from the first k elements and then elements are sampled at the same location in each of the subsequent segments of size k. set.seed(49464) start=sample(1:4,1) start ## [1] 4 The mean GPA of the systematic sample, the sample mean, \\(\\bar{x}=2.77\\) Strengths Assures an even, random sampling of the population. When the population is an ordered list, a systematic sample gives a better representation of the population than a SRS. Can be used in situations where a SRS is difficult or impossible. It is especially useful when the population that you are studying is arranged in time. Weaknesses Weaknesses Not every combination has an equal chance of being selected. Many combinations will never be selected using a systematic sample! Large Variance. Formulas are really complicated Stratified random sample: population is divided into homogeneous groups called strata, then SRS’s are pulled from each strata. set.seed(1837) honors=subset(FakeSchool,Honors==&quot;Yes&quot;) honors ## Students Sex class GPA Honors ## 1 Alice F Fr 3.80 Yes ## 2 Brad M Fr 2.60 Yes ## 8 Andrea F So 4.00 Yes ## 9 Betsy F So 4.00 Yes ## 10 Chris M So 4.00 Yes ## 11 Dylan M So 3.50 Yes ## 15 Adam M Jr 3.98 Yes ## 17 Cassie F Jr 3.75 Yes ## 18 Derek M Jr 3.10 Yes ## 19 Faith F Jr 2.50 Yes ## 22 Angela F Sr 4.00 Yes ## 23 Bob M Sr 3.80 Yes we take a SRS of size 3 from the Honors students: honors.samp=popsamp(3,honors) honors.samp ## Students Sex class GPA Honors ## 15 Adam M Jr 3.98 Yes ## 19 Faith F Jr 2.50 Yes ## 18 Derek M Jr 3.10 Yes set.seed(17365) nonhonors=subset(FakeSchool,Honors==&quot;No&quot;) nonhonors.samp=popsamp(4,nonhonors) nonhonors.samp ## Students Sex class GPA Honors ## 26 Frank M Sr 2.0 No ## 28 Grace F Sr 1.4 No ## 13 Eric M So 2.1 No ## 25 Diana F Sr 2.9 No The sample mean for the stratified sample, \\(\\bar{x}=2.84\\). Strengths: Representative of the population, because elements from all strata are included in the sample. Ensures that specific groups are represented, sometimes even proportionally, in the sample. Since each stratified sample will be distributed similarly, the amount of variability between samples is decreased. Allows comparisons to be made between strata, if necessary. For example, a stratified sample allows you to easily compare the mean GPA of Honors students to the mean GPA of non-Honors students. Weaknesses Requires prior knowledge of the population. You have to know something about the population to be able to split into strata! Cluster Sampling: Cluster sampling is a sampling method used when natural groups are evident in the population. The clusters should all be similar each other: each cluster should be a small scale representation of the population. To take a cluster sample, a random sample of the clusters is chosen. The elements of the randomly chosen clusters make up the sample. View(FakeSchool[order(FakeSchool$class),]) Strengths: Makes it possible to sample if there is no list of the entire population, but there is a list of subpopulations. For example, there is not a list of all church members in the United States. However, there is a list of churches that you could sample and then acquire the members list from each of the selected churches. Weaknesses: Not always representative of the population. Elements within clusters tend to be similar to one another based on some characteristic(s). This can lead to over-representation or under-representation of those characteristics in the sample. Convenience sampling: Ask people who are easy to ask. Produces bias results (Don’t use). 2.17 Identify the sampling design: Modern Managed Hospitals (MMH) is a national for-profit chain of hospitals. Management wants to survey patients discharged this past year to obtain patient satisfaction profiles. They wish to use a sample of such patients. Several sampling techniques are described below. Categorize each as simple random, stratified, systematic, cluster, or convenience sample. Obtain a list of patients discharged from all MMH facilities. Divide the patients according to length of hospital stay (3 days or less, 3 - 7 days, 8 -14 days, more than 14 days). Draw simple random samples from each group. Obtain lists of patients discharged from all MMH facilities. Number these patients, and then use a random number table to obtain the sample. Randomly select some MMH facilities from each of five geographic regions, and then survey all of these hospitals’ discharge lists. At the beginning of the year, instruct each MMH facility to survey every 500th patient discharged. Instruct each MMH facility to survey 10 discharged patients this week and send in the results. 2.18 Experiment Please see class slides. "],
["basic-probilities.html", "Chapter 3 Basic Probilities 3.1 Definitions 3.2 Notation for Probabilities 3.3 Basic Rules for Computing Probability 3.4 Probability Limits 3.5 Complementary Events 3.6 Rule of Complementary Events 3.7 Addition Rule 3.8 Multiplication 3.9 At least one 3.10 Conditional Probability 3.11 Probabilities from two way tables", " Chapter 3 Basic Probilities 3.1 Definitions Event: Any collection of results or outcomes of a procedure Simple Event: an outcome or an event that cannot be further broken down into simpler components Sample Space:for a procedure consists of all possible simple events; that is, the sample space consists of all outcomes that cannot be broken down any further Example Procedure Example of Events Sample Space Single birth 1 girl (simple event) {b, g} 3 births 2 boys and 1 girl (bbg, bgb, and gbb are all simple events) {bbb, bbg, bgb, bgg, gbb, gbg, ggb, ggg} 3.2 Notation for Probabilities P: denotes a probability. A, B, and C: denote specific events. P(A): denotes the probability of event A occurring. 3.3 Basic Rules for Computing Probability Subjective Probability If the probability of an event is based on a personal belief that the event will occur, it is called a subjective probability. Theoretical Probability If the probability of an event is based on reasoning or calculation, it is called a theoretical probability. Calculating Theoretical Probabilities: \\[P(event)=\\frac{\\textrm{# outcomes in the event}}{\\textrm{# outcomes in the sample space}}\\] Long-Run Frequency Probability or Law of Large Numbers If the probability of an event comes from knowing the proportion of times the event occurs when the experiment is performed many times, it is called a long-run frequency probability. 3.4 Probability Limits Always express a probability as a fraction or decimal number between 0 and 1. 1. The probability of an impossible event is 0. 2. The probability of an event that is certain to occur is 1. 3. For any event A, the probability of A is between 0 and 1 inclusive. That is, \\(0\\leq P(A)\\leq 1\\). 3.5 Complementary Events \\[\\textrm{The complement of event A, denoted by} \\bar{A} , \\textrm{consists of all outcomes in which the event A does not occur.}\\] 1010 United States adults were surveyed and 202 of them were smokers. It follows that: \\[P(Smoker)=\\frac{202}{1010}=0.2\\] \\[P(\\textrm{not a smoker})=1-\\frac{202}{1010}=0.8\\] 3.6 Rule of Complementary Events \\[P(A)+P(\\bar{A})=1\\] \\[P(A)=1-P(\\bar{A})\\] \\[P(\\bar{A})=1-P(A)\\] 3.7 Addition Rule Any event combining 2 or more simple events is called a compound event \\[\\textrm{P(A or B) = P(in a single trial, event A occurs or event B occurs or they both occur)}\\] The Math: \\[\\textrm{P(A or B)}=P(A\\cup B)=\\textrm{P(A) + P(B) - P(A and B)}\\] Example: Musical styles other than rock and pop are becoming more popular. A survey of college students finds that the probability they like country music is .40. The probability that they liked jazz is .30 and that they liked both is .10. What is the probability that they like country or jazz? \\[\\textrm{P(Country or Jazz )=P(Country)+P(Jazz)-P(Country and Jazz)=0.4+0.3-0.1=0.6}\\] Disjoint or Mutually Exclusive Events Events A and B are disjoint (or mutually exclusive) if they cannot occur at the same time. (That is, disjoint events do not overlap) Example: A large auto center sells cars made by many different manufacturers. Three of these are Honda, Nissan, and Toyota. (Note: these are not simple events since there are many types of each brand.) Suppose that P(H) = .25, P(N) = .18, P(T) = .14. Are these disjoint events? Yes P(H or N or T) = 0.25+0.18+0.14=0.57 3.8 Multiplication \\[\\textrm{If two events A &amp; B are independent}, P(\\textrm{A and B})=P(A \\cap B)=P(A)\\cdot P(B) \\] \\[\\textrm{If two events A &amp; B are dependent}, P(\\textrm{A and B})=P(A \\cap B)=P(A)\\cdot P(B|A)\\] Example: A certain brand of light bulbs are defective five percent of the time. You randomly pick a package of two such bulbs off the shelf of a store. What is the probability that both bulbs are defective? Can you assume they are independent? Yes. \\[\\textrm{P(Two are defective)=P(1st defective and 2nd defective)=P(1st defective)*P(2nd defective)}=0.05\\cdot 0.05=0.0025\\] There are seven girls and eight boys in a math class. The teacher selects two students at random to answer questions on the board. What is the probability that both students are girls? Are these events independent? No. \\[\\textrm{P(Both girls)=P(1st girl and 2nd girl)}=\\frac{7}{15}\\cdot \\frac{6}{14}=0.2\\] Combing two rules: A certain brand of light bulbs are defective five percent of the time. You randomly pick a package of two such bulbs off the shelf of a store. What is the probability that exactly one bulb is defective? \\[\\textrm{P(exactly one defective)=P(1st defective 2nd good)+P(1st good and 2nd defective)}=0.05\\cdot 0.95+0.95\\cdot 0.05=0.095\\] 3.9 At least one The probability that at least one outcome happens is 1 minus the probability that no outcomes happen. P(at least 1) = 1 - P(none) A certain brand of light bulbs are defective five percent of the time. You randomly pick a package of two such bulbs off the shelf of a store. What is the probability that at least one bulb is defective? 1-.95^2 ## [1] 0.0975 For a sales promotion the manufacturer places winning symbols under the caps of 10% of all Dr. Pepper bottles. You buy a six-pack. What is the probability that you win something? 1-0.9^6 ## [1] 0.468559 3.10 Conditional Probability A probability that takes into account a given condition \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\] In a recent study it was found that the probability that a randomly selected student is a girl is .51 and is a girl and plays sports is .10. If the student is female, what is the probability that she plays sports? 0.1/0.51 ## [1] 0.1960784 The probability that Sam parks in a no-parking zone and gets a parking ticket is 0.06, and the probability that Sam cannot find a legal parking space and has to park in the non-parking zone is 0.2. Find the probability that he will get a parking ticket given that he parks in the no-parking zone. 3.11 Probabilities from two way tables Student Staff Total American 107 105 212 European 33 12 45 Asian 55 47 102 Total 195 164 359 What is the probability that the driver is a student? What is the probability that the driver drives a European car? What is the probability that the driver is staff and drives an Asian car? What is the probability that the driver drives an American or Asian car? What is the probability that the driver is staff or drives an Asian car? If the driver is a student, what is the probability that they drive an American car? What is the probability that the driver is a student if the driver drives a European car? "],
["discrete-random-variables-and-binomial-distribution.html", "Chapter 4 Discrete Random Variables and Binomial Distribution 4.1 Probability Distribution 4.2 Discrete Random Variable 4.3 Continuous Random Variable 4.4 Discrete Probability Distribution 4.5 Example: 4.6 Cumulative sum 4.7 Graph it 4.8 Example: 4.9 Expected value (or mean) and the standard deviation of discrete random variables. 4.10 Fair Game 4.11 Practice", " Chapter 4 Discrete Random Variables and Binomial Distribution A variable (typically represented by x) that has a single numerical value, determined by chance, for each outcome of a procedure 4.1 Probability Distribution A description that gives the probability for each value of the random variable, often expressed in the format of a graph, table, or formula 4.2 Discrete Random Variable either a finite number of values or countable number of values, where “countable” refers to the fact that there might be infinitely many values, but that they result from a counting process. 4.3 Continuous Random Variable has infnitely many values, and those values can be associated with measurements on a continuous scale without gaps or interruptions. 4.4 Discrete Probability Distribution Gives the probabilities associated with each possible x value Usually displayed in a table, but can be displayed with a histogram or formula For every possible x value. \\[0\\leq P(x)\\leq 1\\] For all values of x, \\[\\sum P(x)=1\\] 4.5 Example: Suppose you toss 3 coins &amp; record the number of heads. The random variable X defined as: Create a probability distribution: x 0 1 2 3 P(x) .125 .37 .375 .125 Draw a histogram based on the pdf (Probability Density Function): x.outcomes&lt;-c(0,1,2,3) x.prob&lt;-c(.125,.375, .375, .125) barplot(x.prob, ylim = c(0,0.5), names.arg = x.outcomes, space = 0, xlab=&quot;x&quot;, ylab=&quot;Pr(X=x)&quot;) 4.6 Cumulative sum x.cumul&lt;-cumsum(x.prob) x.cumul ## [1] 0.125 0.500 0.875 1.000 4.7 Graph it barplot(x.cumul, names.arg=x.outcomes, space=0, xlab=&quot;x&quot;, ylab=&quot;Pr(X&lt;=x)&quot;) 0 means P(x less than or equal to 0) or P(x is equal to 0) 1 means P(x less than or equal to 1), which is P(x=0)+P(x=1)=0.125+0.345=0.5 4.8 Example: Let x be the number of courses for which a randomly selected student at a certain university is registered. X 1 2 3 4 5 6 P(X) .02 .03 .09 ? .40 .16 1-(.02+.03+.09+.4+.16+.05) ## [1] 0.25 \\[P(x=4)=.25\\] \\[P(x&lt;4)=.02+.03+.09=.14\\] \\[P(x\\leq 4)=0.14+0.25=0.39\\] What is the probability that the student is registered for at least five courses? 4.9 Expected value (or mean) and the standard deviation of discrete random variables. The expected value of X is the value of X that we would expect to see if we repeated the experiment many times. We can also call it as the “mean” \\[\\mu=E(X)=\\sum x\\cdot P(X)\\] Multiply every value in the range of X by it’s corresponding probability. Sum these products. The standard deviation of X is a measure of how much X is expected to differ from E(X). \\[\\sigma=\\sqrt{\\sum (x-\\mu)^2\\cdot P(X)}\\] Find the difference between every value in the range of XX and EV(X)EV(X). Square the differences. Multiply the squares by the corresponding probability. Sum the products. Take the square root. The variance is the square of the standard deviation, so \\[Var(X)=(x-\\mu)^2\\cdot P(X)\\] Let x be the number of courses for which a randomly selected student at a certain university is registered. X 1 2 3 4 5 6 P(X) .02 .03 .09 .25 .40 .16 Find the expected value (mean) and the standard deviation (Do it by hand first) Using R to find the expected value or the mean x.course=c(1,2,3,4,5,6,7) prob.course=c(.02,.03,.09,.25,.4,.16,.05) mu.x=sum(x.course*prob.course) mu.x ## [1] 4.66 Using R to find the standard deviation: var.x=sum((x.course-mu.x)^2*prob.course) var.x ## [1] 1.4444 sd.x=sqrt(var.x) sd.x ## [1] 1.201832 4.10 Fair Game A fair game is one where the expected net gain is 0 If a player rolls two dice and gets a sum of 2 or 12, he wins $20. If he gets a 7, he wins $5. The cost to roll the dice one time is $3. Is this game fair? x.game=c(-3,2,17) prob.game=c(.78,.17,.05) mu.game=sum(x.game*prob.game) mu.game ## [1] -1.15 4.11 Practice At the College of Warm &amp; Fuzzy, good grades in math are very easy to come by. The grade distribution is given in the table below: Grade Pass Fail Proportion 0.9 0.1 Suppose three students are to be selected at random and their grade is recorded. List the possible outcomes. Define X as the number of students selected who pass the course. Draw the probability distribution. Find the expected number of students who pass the course. Find the standard deviation. You are playing a spinner game for money. If you bet $1 on a number and win, you receive an additional $4. There is a 0.3 probability that you win. What is your expected amount that you will win or lose each time that you play? Is this a fair game? "],
["special-discrete-distribution-binomial-distribution-.html", "Chapter 5 Special Discrete Distribution: Binomial Distribution. 5.1 Conditions: 5.2 Binomial Formula: 5.3 There are 3 cases: 5.4 Binomial Random Variable: Expected value and standard deviation 5.5 Practice", " Chapter 5 Special Discrete Distribution: Binomial Distribution. 5.1 Conditions: Each trial results in one of two mutually exclusive outcomes. (success/failure) There are a fixed number of trials Outcomes of different trials are independent The probability that a trial results in success is the same for all trials The binomial random variable x is defined as the number of successes out of the fixed number Notation: \\[X\\sim \\textrm{Bin(n,p)}\\] Are these binomial distributions? + Toss a coin 10 times and count the number of heads Deal 10 cards from a shuffled deck and count the number of red cards Two parents with genes for O and A blood types and count the number of children with blood type O 5.2 Binomial Formula: \\[P(x=k)=\\dbinom{x}{k}p^k (1-p)^{(n-k)}\\] Where \\[\\dbinom{x}{k}= _{n}C_{k}=\\frac{n!}{k!(n-k)!}\\] 5.3 There are 3 cases: Less than or equal to: Let $Y= $. Find the probability of getting 3 or less heads. pbinomGC(3,region=&quot;below&quot;,size=10, prob=0.5,graph=TRUE) ## [1] 0.171875 Thus \\(P(Y\\leq 3)=0.17\\). Less than equal to is also called at most Less than Find \\(P(Y&lt;3)\\) pbinomGC(2,region=&quot;below&quot;, size=10,prob=0.5,graph=TRUE) ## [1] 0.0546875 Greater Than: Find \\(P(Y&gt;3)\\) pbinomGC(3,region=&quot;above&quot;,size=10, prob=0.5,graph=TRUE) ## [1] 0.828125 Greater than or equal to: Find \\(P(Y\\geq 3)\\) pbinomGC(2,region=&quot;above&quot;,size=10, prob=0.5,graph=TRUE) ## [1] 0.9453125 Greater than or equal to is also called at least Between Find \\(P(2\\leq Y\\leq 4)\\) pbinomGC(c(2,4),region=&quot;between&quot;, size=10,prob=0.5,graph=TRUE) ## [1] 0.3662109 Equal To (or Exact) Find \\(P(Y=4)\\) pbinomGC(c(4,4),region=&quot;between&quot;, size=10,prob=0.5,graph=TRUE) ## [1] 0.2050781 pbinomGC(c(3,3),region=&quot;between&quot;, size=5,prob=0.5,graph=TRUE) ## [1] 0.3125 5.4 Binomial Random Variable: Expected value and standard deviation If \\(X\\sim \\textrm{Bin(n,p)}\\) \\[\\mu=E(X)=n\\cdot p\\] and \\[\\sigma=sd=\\sqrt{np(1-p)}\\] Let $Y= $. Find the mean and standard deviation. 5.5 Practice Out of 3 coins that are tossed, what is the probability of getting exactly 2 heads? pbinomGC(c(2,2),region=&quot;between&quot;, size=3,prob=0.5,graph=TRUE) ## [1] 0.375 The number of inaccurate gauges in a group of four is a binomial random variable. If the probability of a defect gauge is 0.1, what is the probability that only 1 is defective? More than 1 is defective? pbinomGC(c(1,1),region=&quot;between&quot;, size=4,prob=0.1,graph=TRUE) ## [1] 0.2916 pbinomGC(1,region=&quot;above&quot;,size=4, prob=0.1,graph=TRUE) ## [1] 0.0523 A genetic trait of one family manifests itself in 25% of the offspring. If eight offspring are randomly selected, find the probability that the trait will appear in exactly three of them. At least 5? pbinomGC(c(3,3),region=&quot;between&quot;, size=8,prob=.25,graph=TRUE) ## [1] 0.2076416 pbinomGC(4,region=&quot;above&quot;,size=8, prob=.25,graph=TRUE) ## [1] 0.02729797 In a certain county, 30% of the voters are Republicans. If ten voters are selected at random, find the probability that no more than six of them will be Republicans. What is the probability that at least 7 are not Republicans? pbinomGC(6,region=&quot;below&quot;,size=10, prob=.3,graph=TRUE) ## [1] 0.9894079 pbinomGC(6,region=&quot;above&quot;,size=10, prob=0.7,graph=TRUE) ## [1] 0.6496107 In a certain county, 30% of the voters are Republicans. How many Republicans would you expect in ten randomly selected voters? What is the standard deviation for this distribution? 10*.3 ## [1] 3 The mean is 3. sqrt(10*.3*(1-.3)) ## [1] 1.449138 The standard deviation is 1.45 In a certain county, 30% of the voters are Republicans. What is the probability that the number of Republicans out of 10 is within 1 standard deviation of the mean? pbinomGC(c(2,5),region=&quot;between&quot;, size=10,prob=.3,graph=TRUE) ## [1] 0.8033427 "],
["continuous-distributions.html", "Chapter 6 Continuous Distributions 6.1 Continuous random variables 6.2 Density curves 6.3 Example 6.4 Special Continuous Distributions: Uniform Distribution 6.5 Example 6.6 Normal Distributions 6.7 Empirical Rule 6.8 Standard Normal Density Curves 6.9 Strategies for finding probabilities or proportions in normal distributions 6.10 3 cases 6.11 Percentile Ranking 6.12 Practice: 6.13 Normality Check 6.14 Sampling Distribution: 6.15 Central Limit Theorem", " Chapter 6 Continuous Distributions 6.1 Continuous random variables Are numerical variables whose values fall within a range or interval Are measurements Can be described by density curves 6.2 Density curves Is always on or above the horizontal axis Has an area exactly equal to one underneath it Often describes an overall distribution Describe what proportions of the observations fall within each range of values Can be any shape Are generic continuous distributions Probabilities are calculated by finding the area under the curve 6.3 Example Notes: \\[\\textrm{In continuous distributions } P(X &lt; 2)\\&amp;P(X \\leq 2) \\textrm{are the same answer.}\\] 6.4 Special Continuous Distributions: Uniform Distribution \\[X\\sim UNIF(a,b)\\] 1.Is a continuous distribution that is evenly (or uniformly) distributed 2. Has a density curve in the shape of a rectangle 3. Probabilities are calculated by finding the area under the curve \\[\\mu_x=\\frac{a+b}{2}\\\\ \\sigma_x=\\sqrt{\\frac{(b-a)^2}{12}}\\] 6.5 Example The Citrus Sugar Company packs sugar in bags labeled 5 pounds. However, the packaging isn’t perfect and the actual weights are uniformly distributed with a mean of 4.98 pounds and a range of .12 pounds. Construct the uniform distribution above. What is the probability that a randomly selected bag will weigh more than 4.97 pounds? Find the probability that a randomly selected bag weighs between 4.93 and 5.03 pounds. The time it takes for students to drive to school is evenly distributed with a minimum of 5 minutes and a range of 35 minutes. Draw the distribution What is the probability that it takes less than 20 minutes to drive to school? What is the mean and standard deviation of this distribution? 6.6 Normal Distributions \\[X\\sim N(\\mu, \\sigma)\\] Symmetrical bell-shaped (unimodal) density curve Above the horizontal axis The transition points occur at \\(\\mu\\pm\\sigma\\) Probability is calculated by finding the area under the curve As \\(\\sigma\\) increases, the curve flattens &amp; spreads out As \\(\\sigma\\) decreases, the curve gets taller and thinner 6.7 Empirical Rule Approximately 68% of the observations fall within \\(\\sigma\\) of \\(\\mu\\) Approximately 95% of the observations fall within 2\\(\\sigma\\) of \\(\\mu\\) Approximately 99.7% of the observations fall within 3\\(\\sigma\\) of \\(\\mu\\) The heights of college males follow a normal distribution with \\(\\mu=72\\) inches and \\(\\sigma=3\\) Let X= height of a college male. About 95% of college males are between what two heights? 6.8 Standard Normal Density Curves Always has \\(\\mu=0\\) and \\(\\sigma= 1\\) 6.9 Strategies for finding probabilities or proportions in normal distributions State the probability statement Draw a picture Calculate the z-score Look up the probability (proportion) in the table We can combine step 2, 3, 4 using R 6.10 3 cases Greater than P(X&gt;70.9) less than P(X&lt;70.9) pnormGC(70.9,region=&quot;below&quot;,mean=72, sd=3.1,graph=TRUE) ## [1] 0.3613552 between P(69.4&lt;X&lt;79.1) pnormGC(c(69.4,79.1),region=&quot;between&quot;,mean=72, sd=3.1,graph=TRUE) ## [1] 0.7881826 outside P(X&lt;69.4 or X&gt;79.1) 6.11 Percentile Ranking Suppose we want to know the height of a male that is taller than 80% of college men. \\[P(X\\leq x)=0.8\\] 6.12 Practice: The lifetime of a certain type of battery is normally distributed with a mean of 200 hours and a standard deviation of 15 hours. What proportion of these batteries can be expected to last less than 220 hours? pnormGC(220,region=&quot;below&quot;,mean=200, sd=15,graph=TRUE) ## [1] 0.9087888 The lifetime of a certain type of battery is normally distributed with a mean of 200 hours and a standard deviation of 15 hours. What proportion of these batteries can be expected to last more than 220 hours? The lifetime of a certain type of battery is normally distributed with a mean of 200 hours and a standard deviation of 15 hours. How long must a battery last to be in the top 5%? Scores of each of the previous chemistry tests were normally distributed with a mean of 78 and standard deviation of 2.3. Peter will be taking the test tomorrow. What is the probability of Peter getting between 80 and 81 on the test? Scores of each of the previous English tests were normally distributed with a mean of 72 and standard deviation of 4.3. Gina will be taking the test tomorrow. What is the probability of Gina getting at least 61 on the test? Scores of each of the previous physics tests were normally distributed with a mean of 82 and standard deviation of 4.8. Ian will be taking the test tomorrow. What is the probability of Ian getting exactly 78 on the test (think back! No work is necessary. This one is tricky!)? Scores of each of the previous French tests were normally distributed with a mean of 74 and standard deviation of 2.7. Julie will be taking the test tomorrow. What is the probability of Julie getting at most 70 on the test? 6.13 Normality Check Histogram Boxplot (not very accurate) QQ Plot (Quantile Quantile Plot) Use the chicken weight dataset: View(chickwts) histogram(chickwts$weight) qqnorm(chickwts$weight, main=&quot;Normal QQ plot of weights&quot;) qqnorm(chickwts$weight, main=&quot;Normal QQ plot of weights&quot;) qqline(chickwts$weight, col=&quot;red&quot;) Check if the following dataset is normally distributed: 57 61 57 57 58 57 61 54 68 51 49 64 50 48 65 52 56 46 54 49 51 47 55 55 54 42 51 56 55 51 54 51 60 61 43 55 56 61 52 69 64 46 54 fakenumber&lt;-c(57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55, 55, 54, 42, 51, 56, 55 ,51, 54, 51, 60, 61, 43, 55, 56, 61, 52, 69, 64, 46, 54) qqnorm(fakenumber, main=&quot;Normal QQ plot of weights&quot;) qqline(fakenumber, col=&quot;red&quot;) 6.14 Sampling Distribution: The sampling distribution of a statistic is the distribution of values taken by the statistic in all possible samples of the same size from the same population. 6.15 Central Limit Theorem When n is sufficiently large, the sampling distribution of \\(\\bar{x}\\) is well approximated by a normal curve, even when the population distribution is not itself normal. CLT can safely be applied if n exceeds 30. \\[\\bar{x}\\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\] where n is the sample size. Example: The army reports that the distribution of head circumference among soldiers is approximately normal with mean 22.8 inches and standard deviation of 1.1 inches. 1. What is the probability that a randomly selected soldier’s head will have a circumference that is greater than 23.5 inches? What is the probability that a random sample of five soldiers will have an average head circumference that is greater than 23.5 inches? pnormGC(23.5,region=&quot;above&quot;,mean=22.8, sd=1.1/sqrt(5),graph=TRUE) ## [1] 0.07737498 "],
["one-sample-confident-intervals.html", "Chapter 7 One Sample Confident Intervals 7.1 What is the purpose of a confidence interval? 7.2 What happens to your confidence as the interval gets smaller? 7.3 Point Estimate 7.4 Confidence intervals 7.5 Margin of error 7.6 Confidence level 7.7 Critical value 7.8 Confidence level 90%. 7.9 You try finding the critical value for cofidence level 95% and 99% 7.10 Common Critical Values 7.11 Margin of Error for Proportions 7.12 Confidence Interval for Estimating a Population Proportion p 7.13 Steps for doing a confidence interval: 7.14 Assumptions for confidence interval of proportion 7.15 Calculation 7.16 Confidence Interval Mantra (Conclusion): 7.17 Example: 7.18 Assumptions: 7.19 Calculation: 7.20 Conclusion 7.21 Example: 7.22 What about using real datasets? 7.23 Estimate the sample size: 7.24 Confidence Interval: Mean, One Sample 7.25 Student’s t- distribution 7.26 How to find the critical t value? 7.27 Formula: 7.28 Steps for doing a confidence interval for population mean ( steps are the same as confidence interval for proportion): 7.29 Assumptions: 7.30 Examples 7.31 Assumptions: 7.32 Calculations: 7.33 Interpretation: 7.34 Is garlic effective in reducing LDL cholesterol 7.35 You try this: 7.36 Assumptions: 7.37 Calculation 7.38 Interpretation 7.39 Using real dataset (easier) 7.40 You try this: 7.41 Calculation: 7.42 Interpretation: 7.43 Estimate sample sizes: 7.44 Practice", " Chapter 7 One Sample Confident Intervals 7.1 What is the purpose of a confidence interval? To estimate an unknown population parameter 7.2 What happens to your confidence as the interval gets smaller? The smaller the interval, the lower your confidence. 7.3 Point Estimate Use a single statistic based on sample data to estimate a population parameter Simplest approach But not always very precise due to variation in the sampling distribution 7.4 Confidence intervals Are used to estimate the unknown population parameters (population mean, population proportion, or population standard deviation etc.) Formula: \\[\\textrm{point estimate}\\pm \\textrm{margin of error}\\] 7.5 Margin of error Shows how accurate we believe our estimate is The smaller the margin of error, the more precise our estimate of the true parameter Formula: \\[\\textrm{M. E}=\\textrm{critical value}\\cdot \\textrm{standard error of the statistic}\\] 7.6 Confidence level Is the success rate of the method used to construct the interval Using this method, ____% of the time the intervals constructed will contain the true population parameter A confidence level is the probability \\(1-\\alpha\\) (often expressed as the equivalent percentage value) that the confidence interval actually does contain the population parameter, assuming that the estimation process is repeated a large number of times. Most common choices are 90%, \\((\\alpha=0.1)\\), 95%, \\((\\alpha=0.05)\\), or 99%, \\((\\alpha=0.01)\\). 7.7 Critical value \\(z_{\\alpha/2}\\) Found from the confidence level A standard z score can be used to distinguish between sample statistics that are likely to occur and those that are unlikely to occur. Such a z score is called a critical value. The upper z-score with probability p lying to its right under the standard normal curve 7.8 Confidence level 90%. \\(z_{\\alpha/2}=1.645\\). How do we get that number? qnormGC(.95, mean = 0, sd=1, graph = TRUE) ## [1] 1.644854 7.9 You try finding the critical value for cofidence level 95% and 99% 7.10 Common Critical Values confidence level \\(\\alpha\\) \\(z_{\\alpha/2}\\) 90% 0.1 1.645 95% 0.05 1.96 99% 0.01 2.575 7.11 Margin of Error for Proportions The margin of error E is also called the maximum error of the estimate and can be found by multiplying the critical value and the standard deviation of the sample proportions: \\[E=z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] 7.12 Confidence Interval for Estimating a Population Proportion p p: population proportion \\(\\hat{p}\\): sample proportion, or the point estimate n: number of sample values E: margin of error \\(z_{\\alpha/2}\\): z score separating an area of \\(\\alpha/2\\) in the right tail of the standard normal distribution; our critical value 7.13 Steps for doing a confidence interval: Assumptions Calculate the interval Write a statement about the interval in the context of the problem. 7.14 Assumptions for confidence interval of proportion SRS of context normal distribution check: \\(np\\geq 10\\) and \\(n(1-p)\\geq 10\\) Large population size: population size is at least 10n 7.15 Calculation \\[C.I.=\\hat{p}\\pm z_{\\frac{\\alpha}{2}} \\sqrt{\\frac{p(1-p)}{n}}\\] 7.16 Confidence Interval Mantra (Conclusion): We are ___ % confident that the true ___ lies between __.\" Example: We are 99% confident that the true proportion of orange Reese’s Pieces lies between 0.38 and 0.50. 7.17 Example: A May 2000 Gallup Poll found that 38% of a random sample of 1012 adults said that they believe in ghosts. Find a 95% confidence interval for the true proportion of adults who believe in ghosts. 7.18 Assumptions: 7.19 Calculation: prop.test(385, 1012, conf.level = .95) ## ## 1-sample proportions test with continuity correction ## ## data: 385 out of 1012 ## X-squared = 57.392, df = 1, p-value = 3.57e-14 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3505437 0.4112451 ## sample estimates: ## p ## 0.3804348 7.20 Conclusion 7.21 Example: A Pew Research Center poll of 1007 randomly selected adults showed that 85% of respondents know what Twitter is. Find the 95% confidence interval estimate of the population proportion p. How about 90% confidence interval 7.22 What about using real datasets? proptestGC(~sex,data=m111survey, success=&quot;female&quot;, conf.level = .99) ## ## ## Inferential Procedures for a Single Proportion p: ## Variable under study is sex ## Continuity Correction Applied to Test Statistic ## ## ## Descriptive Results: ## ## female n estimated.prop ## 40 71 0.5634 ## ## ## Inferential Results: ## ## Estimate of p: 0.5634 ## SE(p.hat): 0.05886 ## ## 99% Confidence Interval for p: ## ## lower.bound upper.bound ## 0.411766 0.714995 Based on this data, can we conclude that at least 50% of GC students are female? use the dataset “pushups”, find the 95% confidence interval estmate of the true proportion of students prefer “SKILL” position. View(pushups) proptestGC(~position, data=pushups, success=&quot;SKILL&quot;, conf.level = .95) ## ## ## Inferential Procedures for a Single Proportion p: ## Variable under study is position ## Continuity Correction Applied to Test Statistic ## ## ## Descriptive Results: ## ## SKILL n estimated.prop ## 22 30 0.7333 ## ## ## WARNING: Either the number of successes or ## the number of failures is below 10. ## The normal approximation for confidence intervals ## and P-value may be unreliable ## ## ## Inferential Results: ## ## Estimate of p: 0.7333 ## SE(p.hat): 0.08074 ## ## 95% Confidence Interval for p: ## ## lower.bound upper.bound ## 0.575091 0.891576 7.23 Estimate the sample size: When \\(\\hat{p}\\) is known: \\[n=\\frac{[z_{\\alpha/2}]^2 \\hat{p}\\hat{q}}{E^2}\\] When \\(\\hat{p}\\) is unknown: \\[n=\\frac{[z_{\\alpha/2}]^2 0.25}{E^2}\\] Example: Many companies are interested in knowing the percentage of adults who buy clothing online. How many adults must be surveyed in order to be 95% confident that the sample percentage is in error by no more than three percentage points? Use a recent result from the Census Bureau: 66% of adults buy clothing online. 1.96^2*0.66*0.34/0.03^2 ## [1] 957.8389 Assume that we have no prior information suggesting a possible value of the proportion. 1.96^2*0.5*0.5/0.03^2 ## [1] 1067.111 7.24 Confidence Interval: Mean, One Sample require(mosaic) require(tigerstats) 7.25 Student’s t- distribution Developed by William Gosset Continuous distribution Unimodal, symmetrical, bell-shaped density curve Above the horizontal axis Area under the curve equals 1 Based on degrees of freedom 7.26 How to find the critical t value? Use the R command : qt(area, df) Degrees of freedom = n-1 For example: 90% confidence interval, and n=5 qt(.95, df=4) ## [1] 2.131847 You try: 95% confidence interval, and n=15 qt(.975,df=14) ## [1] 2.144787 qt(.975,df=48) ## [1] 2.010635 7.27 Formula: \\[\\textrm{point estimate}\\pm \\textrm{margin of error}\\] In our case: \\[\\bar{x}\\pm t_{\\alpha/2}\\cdot \\frac{s}{\\sqrt{n}}\\] Where \\(\\bar{x}\\) is the sample mean, \\(t_{\\alpha/2}\\) is the critical t value with degress of freedom n-1, \\(s\\) is the sample standard deviation, and n is the sample size. 7.28 Steps for doing a confidence interval for population mean ( steps are the same as confidence interval for proportion): Assumptions Calculate the interval Write a statement about the interval in the context of the problem. 7.29 Assumptions: Have an SRS from population (or randomly assigned treatments) \\(\\sigma\\) unknown Normal (or approx. normal) distribution Given Large sample size Check graph of data 7.30 Examples A common claim is that garlic lowers cholesterol levels. In a test of the effectiveness of garlic, 49 subjects were treated with doses of raw garlic, and their cholesterol levels were measured before and after the treatment. The changes in their levels of LDL cholesterol (in mg/dL) have a mean of 0.4 and a standard deviation of 21.0. Use the sample statistics of \\(n = 49\\), \\(\\bar{x} = 0.4\\), and \\(s = 21.0\\) to construct a 95% confidence interval estimate of the mean net change in LDL cholesterol after the garlic treatment. What does the confidence interval suggest about the effectiveness of garlic in reducing LDL cholesterol? 7.31 Assumptions: SRS \\(\\sigma\\) unknown Normal Distribution: Sample size is greater than 30 7.32 Calculations: sample_mean=0.4 sample_se=21/sqrt(49) # standard error=sample sd/squre root of the sample size sample_mean+c(-1,1)*qt(.975, df=48)*sample_se ## [1] -5.631904 6.431904 7.33 Interpretation: We are 95% confidence that the true mean net change LDL level lies between -5.63 and 6.43 7.34 Is garlic effective in reducing LDL cholesterol Since 0 is included in the confidence interval. There is a high chance the true mean net change is 0, which means there is no change. Thus, garlic is not effecitve in reducing LDL level. 7.35 You try this: A medical researcher measured the pulse rate of a random sample of 20 adults and found a mean pulse rate of 72.69 beats per minute with a standard deviation of 3.86 beats per minute. Assume pulse rate is normally distributed. Compute a 95% confidence interval for the true mean pulse rates of adults. 7.36 Assumptions: SRS of context. \\(\\sigma\\) unknown Normal Distribution given 7.37 Calculation sample_mean=72.69 sample_se=3.86/sqrt(20) # standard error=sample sd/squre root of the sample size sample_mean+c(-1,1)*qt(.975, df=19)*sample_se ## [1] 70.88346 74.49654 7.38 Interpretation We are 95% confidence that the true true mean pulse rates of adults lies between 70.88 and 74.50. 7.39 Using real dataset (easier) ttestGC(~fastest, conf.level = .95,data=m111survey) ## ## ## Inferential Procedures for One Mean mu: ## ## ## Descriptive Results: ## ## variable mean sd n ## fastest 105.9 20.88 71 ## ## ## Inferential Results: ## ## Estimate of mu: 105.9 ## SE(x.bar): 2.478 ## ## 95% Confidence Interval for mu: ## ## lower.bound upper.bound ## 100.959833 110.842984 Would it be reasonable for someone to believe, in the face of this data, that \\(\\mu=100\\)? 7.40 You try this: Consumer Reports tested 14 randomly selected brands of vanilla yogurt and found the following numbers of calories per serving: 160 200 220 230 120 180 140 130 170 190 80 120 100 170 Compute a 98% confidence interval for the average calorie content per serving of vanilla yogurt. Do a complete write up. ## Assumption 1. SRS of context 2. \\(\\sigma\\) unknown 3. Normal Distribution yogurt=c(160, 200, 220, 230, 120, 180, 140, 130, 170, 190, 80, 120, 100, 170 ) boxplot(yogurt, horizontal = TRUE) Based on the incorrect boxplot, it is approximately normally distributed. 7.41 Calculation: ttestGC(~yogurt, conf.level = .98) ## ## ## Inferential Procedures for One Mean mu: ## ## ## Descriptive Results: ## ## variable mean sd n ## yogurt 157.9 44.75 14 ## ## ## Inferential Results: ## ## Estimate of mu: 157.9 ## SE(x.bar): 11.96 ## ## 98% Confidence Interval for mu: ## ## lower.bound upper.bound ## 126.158145 189.556141 7.42 Interpretation: We are 98% confidence that the true mean calorie content per serving of vanilla yogurt lies between 126.16 and 189.56. 7.43 Estimate sample sizes: \\[n=\\left[ \\frac{z_{\\alpha/2}\\cdot\\sigma}{E}\\right ]^2\\] Assume that we want to estimate the mean IQ score for the population of statistics students. How many statistics students must be randomly selected for IQ tests if we want 95% confidence that the sample mean is within 3 IQ points of the population mean? 7.44 Practice In a random sample of 100 people, 40 of them stated they were in favor of the school district bond election. Find a 95% confidence interval for the true proportion of all district residents who favor the bond election. Interpret the interval in the context of the problem. A union member reported that 80 out of 125 workers interviewed supported some sort of work stoppage to further their demands for better safety conditions. Find a 99% confidence interval estimate of the true proportion of workers who would support such action. We are interested in what proportion of the employees of a very large corporation prefer to provide their own retirement benefits in lieu of a company sponsored plan. How many people should be surveyed if we wish to be within 3% of the correct estimate with 95% confidence? What is the critical value (t*) for the following confidence levels with a sample size of 100? 80% confidence 96% confidence 99.8% confidence The following airborne times for United Airlines flight 448 from Albuquerque to Denver on 10 randomly selected days are given below: 57 54 55 51 56 48 52 51 59 59 Compute and interpret a 90% confidence interval for the mean airborne time for flight 448. Based on your interval in part (a), if flight 448 is scheduled to depart at 10:00 A.M., what would you recommend for the published arrival time? Explain. The NAEP (National Assessment of Educational Progress) includes a short test of quantitative skills, covering basic arithmetic and the ability to apply it. Suppose a random sample of 60 young adult men are taken from a large population. If the mean of the sample is 265 and the sample standard deviation is 60, what is a 90% confidence interval? How do the margins of error change as the confidence level increases? "],
["hypothesis-testings.html", "Chapter 8 Hypothesis Testings 8.1 Definitions 8.2 Steps 8.3 Assumptions for testing for proportions (Same as confidence intervals for proportions) 8.4 Writing Hypothesis statements: 8.5 Calculations 8.6 significance level 8.7 Test Statistic 8.8 P-values 8.9 Calculating P-values (By hand) 8.10 Statistical Significant 8.11 Writing Conclusions: 8.12 Conclusion Format: 8.13 Example 8.14 Assumptions 8.15 Hypothesis Statement 8.16 Calculation: 8.17 Conclusion 8.18 More Examples 8.19 Assumptions 8.20 Hypothesis Statement 8.21 Calculation 8.22 Conclusion 8.23 Using real datasets 8.24 Errors 8.25 Type I error 8.26 Type II error 8.27 Example 8.28 Example 8.29 Testing a Claim About a Mean 8.30 Assumptions for t-test: (same as CI for the means) 8.31 Hypothesis Statement: 8.32 Calculation 8.33 Conclusion 8.34 Example: 8.35 Assumptions 8.36 Hypothesis Statement 8.37 Calculation 8.38 Conclusion 8.39 Example 8.40 Assumptions 8.41 Hypothesis Statement 8.42 Calculation 8.43 Example with real datasets 8.44 Assumptions 8.45 Hypothesis Statement 8.46 Calculation 8.47 Conclusion 8.48 Example 8.49 Assumptions: 8.50 Hypothesis Statement 8.51 Calculation", " Chapter 8 Hypothesis Testings 8.1 Definitions A hypothesis is a claim or statement about a property of a population. A hypothesis test is a procedure for testing a claim about a property of a population. Rare Event Rule for Inferential Statistics–If, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct. The null hypothesis (denoted by \\(H_{0}\\)) is a statement that the value of a population parameter (such as proportion, mean, or standard deviation) is equal to some claimed value. We test the null hypothesis directly in the sense that we assume it is true and reach a conclusion to either reject \\(H_{0}\\) or fail to reject \\(H_{0}\\). The alternative hypothesis (denoted by \\(H_{A}\\)) is the statement that the parameter has a value that somehow differs from the null hypothesis. The symbolic form of the alternative hypothesis must use one of these symbols: &lt;, &gt;, \\(\\neq\\). If you are conducting a study and want to use a hypothesis test to support your claim, the claim must be worded so that it becomes the alternative hypothesis. 8.2 Steps Assumptions Hypothesis statements &amp; define parameters Calculations Conclusion, in context 8.3 Assumptions for testing for proportions (Same as confidence intervals for proportions) SRS Normal Distribution np&gt;10 and n(1-p)&gt;10 Large population size. 8.4 Writing Hypothesis statements: Null hypothesis - is the statement being tested; this is a statement of “no effect” or “no difference” (\\(H_{0}\\)) \\[H_{0}:\\textbf{parameter = hypothesized value}\\] Alternative hypothesis - is the statement that we suspect is true (\\(H_{A}\\)) \\[H_{A}: \\textbf{parameter &gt; hypothesized value}\\] \\[H_{A}: \\textbf{parameter &lt; hypothesized value}\\] \\[H_{A}: \\textbf{parameter} \\neq \\textbf{hypothesized value}\\] Fact to remember about hypotheses: ALWAYS refer to populations (parameters) (Use the correct notations.) 8.5 Calculations 8.6 significance level The significance level (denoted by \\(\\alpha\\)) is the probability that the test statistic will fall in the critical region when the null hypothesis is actually true (making the mistake of rejecting the null hypothesis when it is true). Common choices are 0.05, 0.01, and 0.10. If the problem does not specify a value, use 0.05. 8.7 Test Statistic The test statistic is a value used in making a decision about the null hypothesis, and is found by converting the sample statistic to a score with the assumption that the null hypothesis is true. Common formula: \\[\\textbf{Test Statistic}=\\frac{\\textbf{statistic-parameter}}{\\textbf{SE of statistic}}\\] Test statistic for one sample proportion: \\[z=\\frac{\\hat{p}-p}{\\sqrt{\\frac{pq}{n}}}\\] 8.8 P-values Assuming \\(H_{0}\\) is true, the probability that the test statistic would have a value as extreme or more than what is actually observed. Facts about p-values: - ALWAYS make decision about the null hypothesis (\\(H_{0}\\))! - Large p-values show support for the null hypothesis, but never that it is true! - Small p-values show support that the null is not true. (“if the p-value is low the Ho must go”) - Double the p-value for two-tail (=) tests (“things have changed”) - Never accept the null hypothesis! 8.9 Calculating P-values (By hand) Draw &amp; shade a curve &amp; calculate the p-value. z values are the test statistics. 1. right-tail test (Greater than) z = 1.6; p-value=0.0548 2. left-tail test (less than) z = -2.4; P-value = .0082 3. two-tail test (not equal to) z = 2.3; P-value = (.0107)2 = .0214 8.10 Statistical Significant The p-value is as small or smaller than the level of significance, \\(\\alpha\\) If \\(p-value&gt;\\alpha\\), “fail to reject” the null hypothesis at the \\(\\alpha\\) level. If \\(p-value \\leq \\alpha\\), “reject” the null hypothesis at the \\(\\alpha\\) level. 8.11 Writing Conclusions: A statement of the decision being made (reject or fail to reject \\(H_{0}\\)) &amp; why (linkage) A statement of the results in context. (state in terms of \\(H_{A}\\)) 8.12 Conclusion Format: Since the p-value &lt; (&gt;) \\(\\alpha\\), I reject (fail to reject) the \\(H_{0}\\). There is (is not) sufficient evidence to suggest that \\(H_{A}\\).(Be sure to write \\(H_{A}\\) in context (words)!) 8.13 Example The article “Credit Cards and College Students: Who Pays, Who Benefits?” (J. College Student Development (1998): 50-56) described a study of credit card payment practices of college students. According to the authors of the article, the credit card industry asserts that at least 50 % of college students carry a credit card balance from month to month. However, the authors of the article report that, in a random sample of 310 college students, 217 carried a balance each month. Does this sample provide sufficient evidence to reject the industry claim? 8.14 Assumptions SRS of context Normal Distribution: n=310, p=0.5 np=155&gt;10 and n(1-p)=155&gt;10 Large Population Size 8.15 Hypothesis Statement \\[H_{0}=0.5\\\\H_{A}&gt;0.5\\] 8.16 Calculation: prop.test(217, 310, p=0.5, alternative = &quot;greater&quot;, correct=FALSE) # use &quot;less&quot; for &lt; and &quot;two.sided&quot; for not equal to ## ## 1-sample proportions test without continuity correction ## ## data: 217 out of 310 ## X-squared = 49.6, df = 1, p-value = 9.426e-13 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.655609 1.000000 ## sample estimates: ## p ## 0.7 8.17 Conclusion Sine the p-value &lt; \\(\\alpha\\), I reject the \\(H_{0}\\). There is sufficient evdience to suggest that at least 50% of college students carry credit card balance month to month. 8.18 More Examples Children as young as 2 years of age, upon seeing an object placed under a pillow in a familiar setting at home, will understand to look for it after an interval of time and be able to find it. Investigators believe this capability will be less pronounced in a laboratory situation, where the child is away from the familiar setting of home. Let denote the proportion of 2-year-olds than have this understanding in the home situation, and suppose that p = .35. The investigators wish to determine whether the proportion that remembers is less when the child is away from home. A researcher took a sample of 100 students and discovered 30 of them understood to look for an object under a pillow. 8.19 Assumptions SRS of context Normal Distribution. n=100, p=.35 np=35&gt;10 and n(1-p)=65&gt;10 Large Population Size. 8.20 Hypothesis Statement \\[H_{0}=0.35\\\\H_{A}&lt;0.35\\] 8.21 Calculation prop.test(30, 100, p=0.35, alternative = &quot;less&quot;, correct=FALSE) # use &quot;less&quot; for &lt; and &quot;two.sided&quot; for not equal to ## ## 1-sample proportions test without continuity correction ## ## data: 30 out of 100 ## X-squared = 1.0989, df = 1, p-value = 0.1473 ## alternative hypothesis: true p is less than 0.35 ## 95 percent confidence interval: ## 0.0000000 0.3798321 ## sample estimates: ## p ## 0.3 8.22 Conclusion Since the p-value &gt; \\(\\alpha\\), I fail to reject \\(H_{0}\\). There is not sufficient evidence to suggest that the proportion of children remember finding an object under a pillow that is less when the child is away from home. 8.23 Using real datasets Are a majority of Georgetown College Students female? View(m111survey) proptestGC(~sex,data=m111survey, success=&quot;female&quot;,p=0.50, alternative=&quot;greater&quot;, graph=TRUE) ## ## ## Inferential Procedures for a Single Proportion p: ## Variable under study is sex ## Continuity Correction Applied to Test Statistic ## ## ## Descriptive Results: ## ## female n estimated.prop ## 40 71 0.5634 ## ## ## Inferential Results: ## ## Estimate of p: 0.5634 ## SE(p.hat): 0.05886 ## ## 95% Confidence Interval for p: ## ## lower.bound upper.bound ## 0.466564 1.000000 ## ## Test of Significance: ## ## H_0: p = 0.5 ## H_a: p &gt; 0.5 ## ## Test Statistic: z = 0.9571 ## P-value: P = 0.1692 8.24 Errors When you perform a hypothesis test you make a decision: reject \\(H_{0}\\) or fail to reject \\(H_{0}\\). When you make one of these decisions, there is a possibility that you could be wrong! That you made an error! There are two decisions that we make; reject or fail to reject. Each could possibly be a wrong decision; therefore, there are two types of errors. 8.25 Type I error When you reject the null hypothesis that is really true Denoted by \\(\\alpha\\) Is the level of significance of the test 8.26 Type II error When you fail to reject the null hypothesis when it is false Denoted by \\(\\beta\\) \\(H_{0} True\\) \\(H_{0} False\\) Reject \\(H_{0}\\) Type I Good Fail to reject \\(H{0}\\) Good Type II 8.27 Example Lay’s Chip Company decides to accept a truckload of potatoes based upon results from a sample of potatoes from the truckload. What are the hypotheses? Type I error? Type II error? From the supplier’s viewpoint, which is more serious? From the chip company’s viewpoint, which is more serious? 8.28 Example Assume that we are conducting a hypothesis test of the claim that a method of gender selection increases the likelihood of a baby girl, so that the probability of a baby girls is p &gt; 0.5. Here are the null and alternative hypotheses: \\[P=0.5\\\\p&gt;0.5\\] a) Identify a type I error. Identify a type II error. 8.29 Testing a Claim About a Mean 8.30 Assumptions for t-test: (same as CI for the means) Have an SRS of context \\(\\sigma\\) unknown Distribution is approximately normal Given Large sample size (CLT, n&gt;30) Graph data (for smaller sample sizes) 8.31 Hypothesis Statement: Use the notation \\(\\mu\\) 8.32 Calculation The test statistic is \\[t=\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\] 8.33 Conclusion Same format as before. 8.34 Example: The Wall Street Journal (January 27, 1994) reported that based on sales in a chain of Midwestern grocery stores, President’s Choice Chocolate Chip Cookies were selling at a mean rate of $1323 per week. Suppose a random sample of 30 weeks in 1995 in the same stores showed that the cookies were selling at the average rate of $1208 with standard deviation of $275. Does this indicate that the sales of the cookies is different from the earlier figure? 8.35 Assumptions SRS of context \\(\\sigma\\) unknown Normal distribution. \\(n\\geq 30\\) 8.36 Hypothesis Statement \\[H_{0}: \\mu=1323\\\\ H_{A}: \\mu\\neq 1323\\] 8.37 Calculation ttestGC(mean=1208, sd=275, n=30, mu=1323, alternative = &quot;two.sided&quot;, graph = TRUE) ## ## ## Inferential Procedures for One Mean mu: ## ## ## Descriptive Results: ## ## mean sd n ## 1208 275 30 ## ## ## Inferential Results: ## ## Estimate of mu: 1208 ## SE(x.bar): 50.21 ## ## 95% Confidence Interval for mu: ## ## lower.bound upper.bound ## 1105.313312 1310.686688 ## ## Test of Significance: ## ## H_0: mu = 1323 ## H_a: mu != 1323 ## ## Test Statistic: t = -2.29 ## Degrees of Freedom: 29 ## P-value: P = 0.02945 8.38 Conclusion Sine p-value &lt; \\(\\alpha\\), I recject \\(H_{0}\\). There is sufficent evidence to suggest the sales of cookies is different from the earlier figure. 8.39 Example The Fritzi Cheese Company buys milk from several suppliers as the essential raw material for its cheese. Fritzi suspects that some producers are adding water to their milk to increase their profits. Excess water can be detected by determining the freezing point of milk. The freezing temperature of natural milk varies normally, with a mean of -0.545 degrees and a standard deviation of 0.008. Added water raises the freezing temperature toward 0 degrees, the freezing point of water (in Celsius). The laboratory manager measures the freezing temperature of five randomly selected lots of milk from one producer with a mean of -0.538 degrees with standard deviation 0.008. Is there sufficient evidence to suggest that this producer is adding water to his milk? 8.40 Assumptions SRS of context \\(\\sigma\\) unknown Normal distribution given 8.41 Hypothesis Statement \\[H_{0}: \\mu=-0.545 \\\\ H_{A}: \\mu&gt; -0.545\\] 8.42 Calculation ttestGC(mean=-0.538, sd=0.008, n=5, mu=-0.545, alternative = &quot;greater&quot;, graph = TRUE) ## ## ## Inferential Procedures for One Mean mu: ## ## ## Descriptive Results: ## ## mean sd n ## -0.538 0.008 5 ## ## ## Inferential Results: ## ## Estimate of mu: -0.538 ## SE(x.bar): 0.003578 ## ## 95% Confidence Interval for mu: ## ## lower.bound upper.bound ## -0.545627 Inf ## ## Test of Significance: ## ## H_0: mu = -0.545 ## H_a: mu &gt; -0.545 ## ## Test Statistic: t = 1.957 ## Degrees of Freedom: 4 ## P-value: P = 0.06102 ## Conclusion Since p-value &gt; \\(\\alpha\\), I fail to reject \\(H_{0}\\). There is not sufficient evidence to suggest that this producer is adding water to his milk. 8.43 Example with real datasets Listed below are the measured radiation emissions (in W/kg) corresponding to a sample of cell phones. Use a 0.05 level of significance to test the claim that cell phones have a mean radiation level that is less than 1.00 W/kg. 0.38 0.55 1.54 1.55 0.50 0.60 0.92 0.96 1.00 0.86 1.46 8.44 Assumptions SRS of context. \\(\\sigma\\) unknown Normal Distribution because most of the points on the QQ-plot are on the line. cellphone&lt;-c(0.38, 0.55, 1.54, 1.55, 0.50, 0.60, 0.92, 0.96, 1.00, 0.86, 1.46) qqnorm(cellphone) qqline(cellphone) 8.45 Hypothesis Statement \\[H_{0}: \\mu=1.00 \\\\ H_{A}: \\mu&lt; 1.00\\] 8.46 Calculation ttestGC(~cellphone, mu=1.00, alternative = &quot;less&quot;) ## ## ## Inferential Procedures for One Mean mu: ## ## ## Descriptive Results: ## ## variable mean sd n ## cellphone 0.9382 0.4229 11 ## ## ## Inferential Results: ## ## Estimate of mu: 0.9382 ## SE(x.bar): 0.1275 ## ## 95% Confidence Interval for mu: ## ## lower.bound upper.bound ## -Inf 1.169269 ## ## Test of Significance: ## ## H_0: mu = 1 ## H_a: mu &lt; 1 ## ## Test Statistic: t = -0.4849 ## Degrees of Freedom: 10 ## P-value: P = 0.3191 8.47 Conclusion Since p-value&gt;\\(\\alpha\\), I fail to reject \\(H_{0}\\). There is not sufficient evidence to suggest that cell phones have a mean radiation level that is less than 1.00 W/kg. 8.48 Example A random sample of 22 fifth grade pupils have a grade point average of 5.0 in maths with a standard deviation of 0.452, whereas marks range from 1 (worst) to 6 (excellent). The grade point average (GPA) of all fifth grade pupils of the last five years is 4.7. Is the GPA of the 22 pupils different from the populations’ GPA? The following data contain the GPA from 22 pupils. Assume the pupils’ GPA are nomally distributed. 5 5.5 4.5 5 5 6 5 5 4.5 5 5 4.5 4.5 5.5 4 5 5 5.5 4.5 5.5 5 8.49 Assumptions: SRS \\(\\sigma\\) unknown Normal Distribution was given. 8.50 Hypothesis Statement \\[H_{0}: \\mu=4.7 \\\\ H_{A}: \\mu\\neq 4.7\\] 8.51 Calculation GPA&lt;-c(5, 5.5, 4.5, 5, 5, 6, 5, 5, 4.5, 5, 5, 4.5, 4.5, 5.5, 4, 5, 5, 5.5, 4.5, 5.5, 5) ttestGC(~GPA, mu=4.7, alternative = &quot;two.sided&quot;, graph = TRUE) ## ## ## Inferential Procedures for One Mean mu: ## ## ## Descriptive Results: ## ## variable mean sd n ## GPA 4.976 0.4603 21 ## ## ## Inferential Results: ## ## Estimate of mu: 4.976 ## SE(x.bar): 0.1005 ## ## 95% Confidence Interval for mu: ## ## lower.bound upper.bound ## 4.766650 5.185731 ## ## Test of Significance: ## ## H_0: mu = 4.7 ## H_a: mu != 4.7 ## ## Test Statistic: t = 2.749 ## Degrees of Freedom: 20 ## P-value: P = 0.01236 ## Conclusion Since p-value&lt;\\(\\alpha\\), we reject \\(H_{0}\\). There is sufficient evidence to suggest the GPA of the 22 pupils different from the populations’ GPA. "],
["two-sample-inference-with-proportions-and-means.html", "Chapter 9 Two-Sample Inference with Proportions and Means 9.1 Mean of the difference and the standard devaition of the difference. 9.2 Assumptions: 9.3 Calculation: 9.4 Conclusion 9.5 Example 9.6 Assumptions 9.7 Calculation 9.8 Conclusion 9.9 Example 9.10 Hypothesis Testing for Two-Sample Proportions 9.11 Assumptions (Same as confidence intervals) 9.12 Hypothesis Statement (Define your variables) 9.13 Calculation: 9.14 Conclusion 9.15 Example: 9.16 Assumptions 9.17 Hypothesis Statement 9.18 Calculation 9.19 Conclusion 9.20 Use Real Data: 9.21 Two-Sample Inference Procedures with Means 9.22 Confidence Intervals for Two-Sample Mean. 9.23 Assumptions: 9.24 Calculation 9.25 Interpretation 9.26 Example 9.27 Assumptions 9.28 Calculation 9.29 Interpretation. 9.30 Hypothesis Testing for Two-Sample Mean (Difference of Means) 9.31 Assumptions (Same as confidence intervals) 9.32 Hypothesis Statement (Define your variables) 9.33 Interpretation 9.34 Example 9.35 Assumptions 9.36 Hypothesis Statement 9.37 Calculation 9.38 Interpretation 9.39 Real Data 9.40 Matched Pairs Test: A special type of t-inference (Mean Difference) 9.41 Assumptions 9.42 Hypothesis Statement: 9.43 Conclusion 9.44 Example 9.45 Assumptions 9.46 Hypothesis Statement 9.47 Calculation 9.48 Interpretation", " Chapter 9 Two-Sample Inference with Proportions and Means 9.1 Mean of the difference and the standard devaition of the difference. \\[\\mu_{x\\pm Y}=\\mu_{x}\\pm\\mu_{Y}\\\\\\sigma_{X\\pm Y}=\\sqrt{\\sigma_{X}^2+\\sigma_{Y}^2}\\] ## Two Sample Confidence Intervals with Proportions 9.2 Assumptions: Two, independent SRS’s from populations ( or randomly assigned treatments) Populations at least 10n (Or large population size) Normal approximation for both: \\(n_{1}p_{1}\\geq 5\\), \\(n_{1}(1-p_{1})\\geq 5\\), \\(n_{1}p_{2}\\geq 5\\), \\(n_{1}p_{2}\\geq 5\\), 9.3 Calculation: \\[\\textrm{point estimate}\\pm \\textrm{margin of error}\\] The Math: \\[(\\hat{p_1}-\\hat{p_2})\\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p_1}(1-\\hat{p_1})}{n_1}+\\frac{\\hat{p_2}(1-\\hat{p_2})}{n_2}} \\] Where \\(\\hat{p_1}=\\textbf{sample proportion from sample 1}\\), \\(n_1=\\textbf{sample size of sample 1}\\). $x_1=, $\\(p_1=\\textbf{population proportion from population 1}\\). \\(\\hat{p_1}=\\frac{x_1}{n_1}\\) \\(p_2\\),\\(\\hat{p_2}\\), \\(n_2\\), and \\(x_2\\) come from population 2. 9.4 Conclusion We are 95% confident that the true difference in proportion of ... lies between... 9.5 Example At Community Hospital, the burn center is experimenting with a new plasma compress treatment. A random sample of 316 patients with minor burns received the plasma compress treatment. Of these patients, it was found that 259 had no visible scars after treatment. Another random sample of 419 patients with minor burns received no plasma compress treatment. For this group, it was found that 94 had no visible scars after treatment. What is a 95% confidence interval of the difference in proportion of people who had no visible scars between the plasma compress treatment &amp; control group? 9.6 Assumptions Two independent SRS’s Normal Distributions: \\(n_1 p_1=259&gt;5\\), \\(n_1 (1-p_1)=57&gt;5\\), \\(n_2 p_2=94&gt;5\\), \\(n_2 (1-p_2)=325&gt;5\\) Large population sizes. 9.7 Calculation proptestGC(x=c(259,94),n=c(316,419), conf.level = .95) ## ## ## Inferential Procedures for the Difference of Two Proportions p1-p2: ## Results taken from summary data. ## ## ## Descriptive Results: ## ## successes n estimated.prop ## Group 1 259 316 0.8196 ## Group 2 94 419 0.2243 ## ## ## Inferential Results: ## ## Estimate of p1-p2: 0.5953 ## SE(p1.hat - p2.hat): 0.02972 ## ## 95% Confidence Interval for p1-p2: ## ## lower.bound upper.bound ## 0.537030 0.653523 9.8 Conclusion We are 95% confident that the true difference in proportion of people who had no visible scars between the plasma compress treatment &amp; control group lies between 0.54 and 0.65. 9.9 Example Researchers comparing the effectiveness of two pain medications randomly selected a group of patients who had been complaining of a certain kind of joint pain. They randomly divided these people into two groups, and then administered the painkillers. Of the 112 people in the group who received medication A, 84 said this pain reliever was effective. Of the 108 people in the other group, 66 reported that pain reliever B was effective. 1. Construct separate 95% confidence intervals for the proportion of people who reported that the pain reliever was effective. Based on these intervals how do the proportions of people who reported pain relieve with medication A or medication B compare? proptestGC(x=84, n=112, conf.level = .95, correct = FALSE) ## ## ## Inferential Procedures for a Single Proportion p: ## Results based on Summary Data ## ## ## Descriptive Results: ## ## successes n estimated.prop ## 84 112 0.75 ## ## ## Inferential Results: ## ## Estimate of p: 0.75 ## SE(p.hat): 0.04092 ## ## 95% Confidence Interval for p: ## ## lower.bound upper.bound ## 0.669806 0.830194 proptestGC(x=66, n=108, conf.level = 0.95, correct = FALSE) ## ## ## Inferential Procedures for a Single Proportion p: ## Results based on Summary Data ## ## ## Descriptive Results: ## ## successes n estimated.prop ## 66 108 0.6111 ## ## ## Inferential Results: ## ## Estimate of p: 0.6111 ## SE(p.hat): 0.04691 ## ## 95% Confidence Interval for p: ## ## lower.bound upper.bound ## 0.519170 0.703052 Construct a 95% confidence interval for the difference in the proportions of people who may find these medications effective. What did you notice about the intervals? 9.10 Hypothesis Testing for Two-Sample Proportions 9.11 Assumptions (Same as confidence intervals) 9.12 Hypothesis Statement (Define your variables) \\[H_{0}:p_{1}=p_{2} \\textbf{or} p_{1}-p_{2}=0\\\\H_A{}:p_{1}&gt;p_{2} \\textbf{or} p_{1}-p_{2}&gt;0\\\\H_A{}:p_{1}&lt;p_{2} \\textbf{or} p_{1}-p_{2}&lt;0\\\\H_A{}:p_{1}\\neq p_{2} \\textbf{or} p_{1}-p_{2}\\neq 0\\\\\\] 9.13 Calculation: \\[z=\\frac{\\hat{p_1}-\\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})}\\cdot \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] 9.14 Conclusion Similar to one sample. 9.15 Example: A forest in Oregon has an infestation of spruce moths. In an effort to control the moth, one area has been regularly sprayed from airplanes. In this area, a random sample of 495 spruce trees showed that 81 had been killed by moths. A second nearby area receives no treatment. In this area, a random sample of 518 spruce trees showed that 92 had been killed by the moth. Do these data indicate that the proportion of spruce trees killed by the moth is different for these areas? 9.16 Assumptions Two indenpendt SRS’s Normal Distributions \\(n_1 p_1=81&gt;5\\),\\(n_1 (1-p_1)=414&gt;5\\)\\(n_2 p_2=92&gt;5\\), \\(n_2 p_2=426&gt;5\\) Large Populations sizes. 9.17 Hypothesis Statement \\[H_{0}:p_1=p_2\\\\H_{A}:p_1\\neq p_2\\] 9.18 Calculation proptestGC(x=c(81,92), n=c(495,518), p=0, alternative = (&quot;two.sided&quot;), graph = TRUE) ## ## ## Inferential Procedures for the Difference of Two Proportions p1-p2: ## Results taken from summary data. ## ## ## Descriptive Results: ## ## successes n estimated.prop ## Group 1 81 495 0.1636 ## Group 2 92 518 0.1776 ## ## ## Inferential Results: ## ## Estimate of p1-p2: -0.01397 ## SE(p1.hat - p2.hat): 0.02363 ## ## 95% Confidence Interval for p1-p2: ## ## lower.bound upper.bound ## -0.060287 0.032347 ## ## Test of Significance: ## ## H_0: p1-p2 = 0 ## H_a: p1-p2 != 0 ## ## Test Statistic: z = -0.5911 ## P-value: P = 0.5544 9.19 Conclusion Since p-value&gt;\\(\\alpha\\), I fail to reject \\(H_0\\). There is not sufficient evidence to suggest that the proportion of spruce trees killed by the moth is different for these areas. 9.20 Use Real Data: At Georgetown College, who is more likely to believe in love at first sight: a female or a male? View(m111survey) SexLove&lt;- xtabs(~sex+love_first, data=m111survey) SexLove ## love_first ## sex no yes ## female 22 18 ## male 23 8 rowPerc(SexLove) ## love_first ## sex no yes Total ## female 55.00 45.00 100.00 ## male 74.19 25.81 100.00 Hypothesis Testing: (Only do calculation) proptestGC(~sex+love_first, first=&quot;male&quot;, data=m111survey, success=&quot;yes&quot;,p=0, graph=TRUE) ## ## ## Inferential Procedures for the Difference of Two Proportions p1-p2: ## love_first grouped by sex ## ## ## Descriptive Results: ## ## yes n estimated.prop ## male 8 31 0.2581 ## female 18 40 0.4500 ## ## ## WARNING: In at least one of the two groups ## number of successes or number of failures is below 10. ## The normal approximation for confidence intervals ## and P-value may be unreliable. ## ## Inferential Results: ## ## Estimate of p1-p2: -0.1919 ## SE(p1.hat - p2.hat): 0.1112 ## ## 95% Confidence Interval for p1-p2: ## ## lower.bound upper.bound ## -0.409870 0.025999 ## ## Test of Significance: ## ## H_0: p1-p2 = 0 ## H_a: p1-p2 != 0 ## ## Test Statistic: z = -1.726 ## P-value: P = 0.08432 9.21 Two-Sample Inference Procedures with Means 9.22 Confidence Intervals for Two-Sample Mean. 9.23 Assumptions: Have two SRS’s from the populations or two randomly assigned treatment groups Samples are independent Both distributions are approximately normally Have large sample sizes Graph BOTH sets of data \\(\\sigma&#39;s\\) unknown. 9.24 Calculation \\[(\\bar{x_1}-\\bar{x_2})\\pm t_{\\alpha/2}\\cdot \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\] 9.25 Interpretation We are 95% confident that the true difference in mean … lies between … 9.26 Example Two competing headache remedies claim to give fast-acting relief. An experiment was performed to compare the mean lengths of time required for bodily absorption of brand A and brand B. Assume the absorption time is normally distributed. Twelve people were randomly selected and given an oral dosage of brand A. Another 12 were randomly selected and given an equal dosage of brand B. The length of time in minutes for the drugs to reach a specified level in the blood was recorded. The results follow: Brand mean SD n Brand A 20.1 8.7 12 Brand B 18.9 7.5 12 Find a 95% confidence interval difference in mean lengths of time required for bodily absorption of each brand. 9.27 Assumptions Two independent SRS’s. Normal distributions given \\(\\sigma&#39;s\\) unknown. 9.28 Calculation ttestGC(mean=c(20.1, 18.9), sd=c(8.7, 7.5), n=c(12,12), conf.level = 0.95) ## ## ## Inferential Procedures for the Difference of Two Means mu1-mu2: ## (Welch&#39;s Approximation Used for Degrees of Freedom) ## Results from summary data. ## ## ## Descriptive Results: ## ## group mean sd n ## Group 1 20.1 8.7 12 ## Group 2 18.9 7.5 12 ## ## ## Inferential Results: ## ## Estimate of mu1-mu2: 1.2 ## SE(x1.bar - x2.bar): 3.316 ## ## 95% Confidence Interval for mu1-mu2: ## ## lower.bound upper.bound ## -5.685410 8.085410 9.29 Interpretation. We are 95% confident that the true mean lengths of time required for bodily absorption of each brand lies between -5.69 and 8.09. 9.30 Hypothesis Testing for Two-Sample Mean (Difference of Means) 9.31 Assumptions (Same as confidence intervals) 9.32 Hypothesis Statement (Define your variables) \\[H_{0}:\\mu_{1}=\\mu_{2} \\textbf{or} \\mu_{1}-\\mu_{2}=0\\\\H_A{}:\\mu_{1}&gt;\\mu_{2} \\textbf{or} \\mu_{1}-\\mu_{2}&gt;0\\\\H_A{}:\\mu_{1}&lt;\\mu_{2} \\textbf{or} \\mu_{1}-\\mu_{2}&lt;0\\\\H_A{}:\\mu_{1}\\neq \\mu_{2} \\textbf{or} \\mu_{1}-\\mu_{2}\\neq 0\\\\\\] ## Calculation: \\[t=\\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\\] 9.33 Interpretation Similar to one sample. 9.34 Example Two competing headache remedies claim to give fast-acting relief. An experiment was performed to compare the mean lengths of time required for bodily absorption of brand A and brand B. Assume the absorption time is normally distributed. Twelve people were randomly selected and given an oral dosage of brand A. Another 12 were randomly selected and given an equal dosage of brand B. The length of time in minutes for the drugs to reach a specified level in the blood was recorded. The results follow: Brand mean SD n Brand A 20.1 8.7 12 Brand B 18.9 7.5 12 Is there sufficient evidence that these drugs differ in the speed at which they enter the blood stream? 9.35 Assumptions Two independent SRS’s. Normal distributions given \\(\\sigma&#39;s\\) unknown. 9.36 Hypothesis Statement \\[\\mu_A=\\mu_B\\\\ \\mu_A\\neq \\mu_B\\] 9.37 Calculation ttestGC(mean=c(20.1, 18.9), sd=c(8.7, 7.5), n=c(12,12), mu=0, graph = TRUE) ## ## ## Inferential Procedures for the Difference of Two Means mu1-mu2: ## (Welch&#39;s Approximation Used for Degrees of Freedom) ## Results from summary data. ## ## ## Descriptive Results: ## ## group mean sd n ## Group 1 20.1 8.7 12 ## Group 2 18.9 7.5 12 ## ## ## Inferential Results: ## ## Estimate of mu1-mu2: 1.2 ## SE(x1.bar - x2.bar): 3.316 ## ## 95% Confidence Interval for mu1-mu2: ## ## lower.bound upper.bound ## -5.685410 8.085410 ## ## Test of Significance: ## ## H_0: mu1-mu2 = 0 ## H_a: mu1-mu2 != 0 ## ## Test Statistic: t = 0.3619 ## Degrees of Freedom: 21.53 ## P-value: P = 0.721 9.38 Interpretation Sine p-value&gt;\\(\\alpha\\), I fail to reject \\(H_0\\). There is not sufficient evidence to suggest that these drugs differ in the speed at which they enter the blood stream. 9.39 Real Data Does the data provide strong evidence that GC males drive faster, on average, than GC females do? ttestGC(fastest~sex, first=&quot;male&quot;, data=m111survey, mu=0,alternative=&quot;greater&quot;, graph=TRUE) ## ## ## Inferential Procedures for the Difference of Two Means mu1-mu2: ## (Welch&#39;s Approximation Used for Degrees of Freedom) ## fastest grouped by sex ## ## ## Descriptive Results: ## ## group mean sd n ## male 113.5 22.57 31 ## female 100.0 17.61 40 ## ## ## Inferential Results: ## ## Estimate of mu1-mu2: 13.4 ## SE(x1.bar - x2.bar): 4.918 ## ## 95% Confidence Interval for mu1-mu2: ## ## lower.bound upper.bound ## 5.175636 Inf ## ## Test of Significance: ## ## H_0: mu1-mu2 = 0 ## H_a: mu1-mu2 &gt; 0 ## ## Test Statistic: t = 2.725 ## Degrees of Freedom: 55.49 ## P-value: P = 0.004289 Sine p-value&lt;\\(\\alpha\\), I reject \\(H_0\\). There is sufficient evidence to suggest GC males drive faster, on average, than GC females do. 9.40 Matched Pairs Test: A special type of t-inference (Mean Difference) One form Pair individuals by certain characteristics Randomly select treatment for individual A Individual B is assigned to other treatment Assignment of B is dependent on assignment of A Second form Individual persons or items receive both treatments Order of treatments are randomly assigned or before &amp; after measurements are taken The two measures are dependent on the individual 9.41 Assumptions Same as two-sample t test, but they are denpendent. 9.42 Hypothesis Statement: \\[H_{0}: \\mu_{d}=0\\\\H_{A}: \\mu_{d}&gt;0\\\\H_{A}: \\mu_{d}&lt;0\\\\H_{A}: \\mu_{d}\\neq 0\\] ## Run a 1 sample t-test (mean differences)!! (not two-sample) 9.43 Conclusion Same as before. 9.44 Example A whale-watching company noticed that many customers wanted to know whether it was better to book an excursion in the morning or the afternoon. To test this question, the company collected the following data on 15 randomly selected days over the past month. (Note: days were not consecutive.) Is there sufficient evidence to suggest more excursions are booked in the afternoon? Day 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Mornig 8 9 7 9 10 13 10 8 2 5 7 7 6 8 7 After Noon 8 10 9 8 9 11 8 10 4 7 8 9 6 6 9 9.45 Assumptions Two dependent SRS’s. \\(\\sigma&#39;s\\) unknown. Normal Distribution. morning&lt;-c(8 ,9, 7, 9, 10, 13, 10, 8, 2, 5, 7, 7, 6, 8, 7) afternoon&lt;-c(8 ,10, 9, 8, 9, 11, 8, 10, 4 ,7 ,8 ,9 ,6 ,6,9) diff&lt;-morning-afternoon qqnorm(diff) qqline(diff) The difference is apprioximately normally distributed because most of the points are closed to the line. 9.46 Hypothesis Statement \\[H_0:\\mu_{m-a}=0\\\\ H_A: \\mu_{m-a}&lt;0\\] 9.47 Calculation ttestGC(~morning-afternoon, mu=0, alternative = &quot;less&quot;) ## ## ## Inferential Procedures for the Difference of Means mu-d: ## morning minus afternoon ## ## ## Descriptive Results: ## ## Difference mean.difference sd.difference n ## morning - afternoon -0.4 1.639 15 ## ## ## Inferential Results: ## ## Estimate of mu-d: -0.4 ## SE(d.bar): 0.4231 ## ## 95% Confidence Interval for mu-d: ## ## lower.bound upper.bound ## -Inf 0.345281 ## ## Test of Significance: ## ## H_0: mu-d = 0 ## H_a: mu-d &lt; 0 ## ## Test Statistic: t = -0.9453 ## Degrees of Freedom: 14 ## P-value: P = 0.1803 9.48 Interpretation Since p-value &gt;\\(\\alpha\\), I fail to reject \\(H_0\\). There is not sufficient evidence to suggest more excursions are booked in the afternoon. "],
["linear-regression.html", "Chapter 10 Linear Regression 10.1 Correlation 10.2 Identify as having a positive association, a negative association, or no association. 10.3 Correlation Coefficient: r 10.4 Requirements for Linear Correlation 10.5 Properties of r 10.6 Interpretation of r 10.7 Antother Example: 10.8 Practice 10.9 Linear Regression 10.10 Bivariate data 10.11 Regression 10.12 Least Squares Regression Line: LSRL 10.13 Interpretations 10.14 Example 10.15 Extrapolation 10.16 Use A Dataset to practice 10.17 Residuals (error) 10.18 Resudual Plot 10.19 Get Residual Plots 10.20 Coefficient of determination 10.21 Influential point 10.22 Which of these measures are resistant? 10.23 Typical Computer output (using the “cars” dataset) 10.24 Practice", " Chapter 10 Linear Regression 10.1 Correlation A correlation exists between two variables when the values of one are somehow associated with the values of the other in some way. A linear correlation exists between two variables when there is a correlation and the plotted points of paired data result in a pattern that can be approximated by a straight line. Suppose we found the age and weight of a sample of 10 adults. Create a scatterplot of the data below. Is there any relationship between the age and weight of these adults? Age 24 30 41 28 50 46 49 35 20 39 Wt 256 124 320 185 158 129 103 196 110 130 age&lt;-c(24, 30, 41, 28, 50, 46, 49, 35, 20, 39) weight&lt;-c(256, 124, 320, 185, 158, 129, 103, 196, 110, 130) xyplot(weight~age, xlab=&quot;Age of Adults&quot;, ylab=&quot;Weight in Lb&quot;, main=&quot;Weight V.S Age&quot;, col=&quot;blue&quot;,pch=19) Suppose we found the height and weight of a sample of 10 adults. Create a scatterplot of the data below. Is there any relationship between the height and weight of these adults? Is it positive or negative? Weak or strong? Ht 74 65 77 72 68 60 62 73 61 64 Wt 256 124 320 185 158 129 103 196 110 130 height&lt;-c(74, 65, 77, 72, 68, 60, 62, 73, 61, 64 ) weight&lt;-c(256, 124, 320, 185, 158, 129, 103, 196, 110, 130) xyplot(weight~height, xlab=&quot;Height of adults&quot;, ylab=&quot;Weight of adults&quot;, main=&quot;Weight V.S Height&quot;, col=&quot;red&quot;, pch=16) 10.2 Identify as having a positive association, a negative association, or no association. Heights of mothers &amp; heights of their adult daughters Age of a car in years and its current value Weight of a person and calories consumed Height of a person and the person’s birth month Number of hours spent in safety training and the number of accidents that occur. 10.3 Correlation Coefficient: r A quantitative assessment of the strength &amp; direction of the linear relationship between bivariate, quantitative data Pearson’s sample correlation is used most parameter - \\(\\rho\\) statistic - r \\[r=\\frac{1}{n-1}\\sum(\\frac{x_i-\\bar{x}}{s_x})(\\frac{y_i-\\bar{y}}{s_y})\\] 10.4 Requirements for Linear Correlation The sample of paired (x, y) data is a simple random sample of quantitative data. Visual examination of the scatterplot must confirm that the points approximate a straight-line pattern. The outliers must be removed if they are known to be errors. The effects of any other outliers should be considered by calculating r with and without the outliers included. 10.5 Properties of r legitimate values of r is \\(-1\\leq r\\leq 1\\) Strong positive correlation: \\(0.8\\leq r\\leq 1\\) Strong negative correlation: \\(-1\\leq r\\leq -0.8\\) Moderate positive correlation: \\(0.5\\leq r\\leq 0.8\\) Moderate negative correlation: \\(-0.8\\leq r\\leq -0.5\\) Weak positive correlation: \\(0&lt; r\\leq 0.5\\) Weak negative correlation: \\(-0.5&lt; r&lt; 0\\) No correlation: \\(r\\approx 0\\) If all values of either variable are converted to a different scale, the value of r does not change The value of r is not affected by the choice of x and y. Interchange all x- and y-values and the value of r will not change. r measures strength of a linear relationship. r is very sensitive to outliers, which can dramatically affect the value of r. 10.6 Interpretation of r There is a direction, strength, type of association between x and y. Speed Limit(mph) 55 50 45 40 30 20 Avg. # of accidents(per week) 28 25 21 17 11 6 Calculate r. Interpret r in context. speed&lt;-c(55, 50, 45, 40, 30, 20) accidents&lt;-c(28, 25, 21, 17, 11, 6) xyplot(accidents~speed) cor(accidents~speed, use=&quot;na.or.complete&quot;) ## [1] 0.9963587 Interpretation: There is a positve, strong, linear association between the speed limit and the number of accidents occured per week. 10.7 Antother Example: Find the correlation for these points: x -3 -1 1 3 5 7 9 Y 40 20 8 4 8 20 40 What does this correlation mean? Sketch the scatterplot 10.8 Practice The paired shoe / height data from five males are listed below. Find and interpret the value of the correlation coefficient r. Shoe print(cm) 29.7 29.7 31.4 31.8 27.6 Height (cm) 175.3 177.8, 185.4 175.3 172.7 shoe_print&lt;-c(29.7, 29.7, 31.4, 31.8, 27.6) height&lt;-c(175.3, 177.8, 185.4, 175.3, 172.7) xyplot(height~shoe_print) cor(height~shoe_print,use=&quot;na.or.complete&quot; ) ## [1] 0.5912691 There is a positive, moderate, linear association (correlation) between the shoe print size and the height. Correlation does not imply causation!!!!!!!!! Correlation does not imply causation!!!!!!!!! Correlation does not imply causation!!!!!!!!! 10.9 Linear Regression 10.10 Bivariate data x - variable: is the independent or explanatory variable y- variable: is the dependent or response variable Use x to predict y 10.11 Regression The regression equation expresses a relationship between x (called the explanatory variable, predictor variable or independent variable), and \\(\\hat{y}\\) (called the response variable or dependent variable). Typical equation: \\[\\hat{y}=a+bx\\] where + \\(\\hat{y}\\) is the predicted y value + \\(b\\) is the slope. it is the approximate amount by which y increases when x increases by 1 unit + \\(a\\) is the y-intercept. it is the approximate height of the line when \\(x =0\\). Some situations, the y-intercept has no meaning 10.12 Least Squares Regression Line: LSRL The line that gives the best fit to the data set The line that minimizes the sum of the squares of the deviations from the line 10.13 Interpretations Define x and \\(\\hat{y}\\) Slope: For each unit increase in x, there is an approximate increase/decrease of b in y. y-intercept: The value of y when \\(x=0\\). Write it in context. Correlation coefficient (r): There is a direction, strength, linear of association between x and y. 10.14 Example The ages (in months) and heights (in inches) of seven children are given. x 16 24 42 60 75 102 120 y 24 30 35 40 48 56 60 age&lt;-c(16, 24, 42, 60, 75, 102, 120) height_kids&lt;-c(24, 30, 35, 40, 48, 56, 60) lsrl_kids&lt;-lmGC(height_kids~age, graph = TRUE) lsrl_kids ## ## Linear Regression ## ## Correlation coefficient r = 0.9941 ## ## Equation of Regression Line: ## ## height_kids = 20.4036 + 0.3421 * age ## ## Residual Standard Error: s = 1.5958 ## R^2 (unadjusted): R^2 = 0.9882 Find the LSRL. Define x and \\(\\hat{y}\\). \\[\\hat{y}=20.40+0.34x\\] x is the age of children in months. \\(\\hat{y}\\) is the predicted height of children in inches. Interpret the slope, y-intercept and correlation coefficient in the context of the problem. Slope: For each month increas in the age of children, there is approximately increase of 0.34 inches in children’s heights. y-intercept: When a baby was born, his/her predicted height was 20.40 inches. correlation coefficient: There is a strong, postive, linar association between the age of chidren and the height of children. Predict the height of a child who is 4.5 years old. predict(lsrl_kids, x=54) ## Predict height_kids is about 38.88, ## give or take 1.712 or so for chance variation. The predicted height of a child who is 4.5 years old is 38.88 inches. Predict the height of someone who is 20 years old. predict(lsrl_kids, x=240) ## Predict height_kids is about 102.5, ## give or take 3.422 or so for chance variation. 10.15 Extrapolation The LSRL should not be used to predict y for values of x outside the data set. It is unknown whether the pattern observed in the scatterplot continues outside this range. 10.16 Use A Dataset to practice require(MASS) View(survey) ?survey Plot the student heights on the y-axis and their handspans (of their writing hand) on the x-axis. LSRL_Height&lt;-lmGC(Height~Wr.Hnd, data=survey, graph = TRUE) LSRL_Height ## ## Linear Regression ## ## Correlation coefficient r = 0.601 ## ## Equation of Regression Line: ## ## Height = 113.9536 + 3.1166 * Wr.Hnd ## ## Residual Standard Error: s = 7.9088 ## R^2 (unadjusted): R^2 = 0.3612 Find the LSRL of student height by handspan. \\[\\hat{y}=113.95+3.126x\\] Where x is the handspans (of their writing hand) in cm, and \\(\\hat{y}\\) is the prediceted heights in cm. Find correlation coefficient and interpret the result. There is moderate, positive, linear association between handspans and heights. Interpret the slope and y-intercept. Slope: for each centermeter increase in handspan, there is approximately increase of 3.12 cm in height. y-intercept: when the handspan is 0 cm, there predicted height is 113.95 cm. 10.17 Residuals (error) The vertical deviation between the observations &amp; the LSRL the sum of the residuals is always zero error = observed - expected \\[residual = y-\\hat{y}\\] 10.18 Resudual Plot A scatterplot of the (x, residual) pairs. Residuals can be graphed against other statistics besides x Purpose is to tell if a linear association exist between the x &amp; y variables If no pattern exists between the points in the residual plot, then the association is linear. 10.19 Get Residual Plots LSRL_Height&lt;-lm(Height~Wr.Hnd, data=survey) plot(LSRL_Height) 10.20 Coefficient of determination \\(r^2\\) gives the proportion of variation in y that can be attributed to an approximate linear relationship between x &amp; y remains the same no matter which variable is labeled x Interpretation: Approximately \\(r^2\\%\\) of the variation in y can be explained by the LSRL of x &amp; y. 10.21 Influential point A point that influences where the LSRL is located If removed, it will significantly change the slope of the LSRL 10.22 Which of these measures are resistant? LSRL Correlation coefficient Coefficient of determination 10.23 Typical Computer output (using the “cars” dataset) View(cars) ?cars xyplot(dist~speed, data=cars, xlab=&quot;Speed of Cars Per Hour&quot;, ylab=&quot;Stopping Distance&quot;, main=&quot;Scatter Plot of Stopping Distance V.S Speed&quot;) LSRL_Cars&lt;-lm(cars$dist~cars$speed) summary(LSRL_Cars) ## ## Call: ## lm(formula = cars$dist ~ cars$speed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 plot(resid(LSRL_Cars)) 10.24 Practice An analysis of the Acme Trucking Corporation’s bills of lading for the freight charge for a standardsized crate reveals the following: Destination Distance (hundreds of miles) 8 11 13 16 18 19 Charge (to nearest dollar) 50 62 65 70 75 80 Compute and interpret the value of the correlation coefficient in context of the problem. DDistance&lt;-c(8, 11, 13, 16, 18, 19) Charge&lt;-c(50, 62, 65, 70, 75, 80) LSRL_Charge&lt;-lmGC(Charge~DDistance, graph = TRUE) LSRL_Charge ## ## Linear Regression ## ## Correlation coefficient r = 0.9843 ## ## Equation of Regression Line: ## ## Charge = 32.3761 + 2.444 * DDistance ## ## Residual Standard Error: s = 2.0871 ## R^2 (unadjusted): R^2 = 0.9689 There is a postive, strong, linear assocation between destination distance and charge. Compute the LSRL. Interpret the slope and y-intercept in context of the problem. \\[\\hat{y}=32.38+2.44x\\] x is the destinatinon distance and \\(\\hat{y}\\) is the predicted charges. Slope: For each mile increase in the destination distance, there is approximately $2.44 increase in charges. y-intercept: When the truck doesn’t move, the predicted charge is $32.38. Predict the charge when the destination distance is 12 hundreds of miles. predict(LSRL_Charge, x=12) ## Predict Charge is about 61.7, ## give or take 2.304 or so for chance variation. The predicted charge is about $61.7. Find \\(r^2\\) and interpret the result. \\(r^2=0.97\\): Approximately 97% of variation in charges can be explained by the LSRL of destination distance and charges. Is this an appropriate equation for this data? Explain. plot(LSRL_Charge) Based on the scatter plot, there is a linear assocation between destination distance and charges. Also, there is “no” pattern shown on the residual plot. As a result, this is an appropriate equation for this dataset. "],
["chi-squared-test.html", "Chapter 11 Chi-Squared Test 11.1 Testing Categorical Variables or Contingency Tables– \\(\\chi^2\\) Test 11.2 Chi-Squared Test (\\(\\chi^2\\)) 11.3 \\(\\chi^2\\) Distribution 11.4 \\(\\chi^2\\) Assumptions 11.5 Hypothesis Statetment: Write in Context 11.6 First Case: \\(\\chi^2\\) Goodness-of-Fit Test 11.7 How do we find the expected counts 11.8 Example 11.9 Full Write-up 11.10 Assumptions 11.11 Hypothesis Statement 11.12 Conclusion 11.13 Example 11.14 Example 11.15 Assumptions 11.16 Hypothesis Statement 11.17 Calculation 11.18 Conclusion 11.19 \\(\\chi^2\\) test for independence 11.20 Example 11.21 Better Way to find the Extected Counts 11.22 Question 11.23 Assumptions 11.24 Hypothesis Statement 11.25 Calculation 11.26 Interpretation 11.27 \\(\\chi^2\\) Test for Homogenity 11.28 Assumptions 11.29 Hypothesis Statement 11.30 Calculation 11.31 Conclusion 11.32 Use A Real Dataset", " Chapter 11 Chi-Squared Test 11.1 Testing Categorical Variables or Contingency Tables– \\(\\chi^2\\) Test What if we are interested in seeing if a candy manufacture’s process actually produces equal amounts of each color. What can I do? 11.2 Chi-Squared Test (\\(\\chi^2\\)) Used to test the counts of categorical data Three Types Goodness of fit (univariate) Independence (bivariate) Homogeneity (univerate with two samples) 11.3 \\(\\chi^2\\) Distribution x&lt;-seq(0,20, length=100) plot(x, dchisq(x, df=1), type=&quot;l&quot;, xlim=c(0,15), ylim=c(0,0.5), ylab=&quot;Density&quot;) lines(x, dchisq(x, df=5), lty=2) lines(x, dchisq(x, df=10), lty=3) abline(h=0, col=&quot;gray&quot;) abline(v=0, col=&quot;gray&quot;) legend(&quot;topright&quot;, legend=c(&quot;df=1&quot;, &quot;df=5&quot;, &quot;df=10&quot;), lty=1:3) * Different df have different curves * Skewd to the right * As df increases, curve shifts toward right and becomes more like a normal curve. 11.4 \\(\\chi^2\\) Assumptions SRS All expected counts are greater than or equal to 5 Observations are mutually exclusive and independent. 11.5 Hypothesis Statetment: Write in Context \\[H_0: \\textrm{The observed counts is equal the expected counts}\\\\H_A: \\textrm{The observed counts is not equal the expected counts} \\] Or use math symbols: \\[H_{0}: p_1=p_2\\cdots=p_n\\\\H_A: \\textrm{At least one proportion is different from the others}\\] ## \\(\\chi^2\\) formula \\[\\chi^2=\\sum\\frac{(Observed-Expected)^2}{Expected}\\] or \\[\\chi^2=\\sum\\frac{(O-E)^2}{E}\\] 11.6 First Case: \\(\\chi^2\\) Goodness-of-Fit Test Uses univariate data Want to see how well the observed counts fit what we expect the counts to be. df=number of categories-1 Same procedures as hypothesis testings 11.7 How do we find the expected counts If categories have the same proportion, use \\(E=\\frac{n}{k}\\), where n is the total counts, and k is the number of categories. If categories have different proportions, use \\(E=np\\), where n is the total counts, and p is the proportion for each category. 11.8 Example Does your zodiac sign determine how successful you will be? Fortune magazine collected the zodiac signs of 256 heads of the largest 400 companies. Is there sufficient evidence to claim that successful people are more likely to be born under some signs than others? Counts Aries Taurus Gemini Cancer Leo Virgo Libra Scorpio Sagittarius Capricorn Aquarius Pisces Observed 23 20 18 23 18 21 19 22 20 19 24 29 Expected How many would you expect in each sign if there were no difference between them? How many degrees of freedom? 11.9 Full Write-up 11.10 Assumptions SRS All expected counts \\(\\geq 5\\) 11.11 Hypothesis Statement \\[H_0: \\text{The number of CEO&#39;s born under each sign is the same}\\\\H_A:\\text{The number of CEO&#39;s born under each sign is different} \\] ## Calculation: + Traditional Way \\[ \\chi^2=\\sum\\frac{(O-E)^2}{E}=\\frac{(23-21.3)^2}{21.3}+\\frac{(20-21.3)^2}{21.3}+\\cdots +\\frac{(29-21.3)^2}{21.3}=5.094\\] get the p-value( Always use the right-tail test) # Set up a table zodiac_CEO&lt;-c(Aries=23, Taurus=20, Gemini=18, Cancer=23, Leo=18, Virgo=21, Libra=19, Scorpio=22, Sagittarius=20, Capricorn=19, Aquarius=24, Pisces=29) zodiac_CEO ## Aries Taurus Gemini Cancer Leo Virgo ## 23 20 18 23 18 21 ## Libra Scorpio Sagittarius Capricorn Aquarius Pisces ## 19 22 20 19 24 29 # get expected, use proportion, not the observed counts!! observed_CEO&lt;-rep(1/12,12) chisqtestGC(zodiac_CEO, p=observed_CEO, graph = TRUE) ## Chi-squared test for given probabilities ## ## Observed counts Expected by Null Contr to chisq stat ## Aries 23 21.33 0.13 ## Taurus 20 21.33 0.08 ## Gemini 18 21.33 0.52 ## Cancer 23 21.33 0.13 ## Leo 18 21.33 0.52 ## Virgo 21 21.33 0.01 ## Libra 19 21.33 0.26 ## Scorpio 22 21.33 0.02 ## Sagittarius 20 21.33 0.08 ## Capricorn 19 21.33 0.26 ## Aquarius 24 21.33 0.33 ## Pisces 29 21.33 2.76 ## ## ## Chi-Square Statistic = 5.0938 ## Degrees of Freedom of the table = 11 ## P-Value = 0.9265 11.12 Conclusion Since p-value&gt;\\(\\alpha\\), I fail to reject to \\(H_0\\). There is not sufficient evidence to suggest that the number of CEO’s born under each sign is different. 11.13 Example A company says its premium mixture of nuts contains 10% Brazil nuts, 20% cashews, 20% almonds, 10% hazelnuts and 40% peanuts. You buy a large can and separate the nuts. Upon weighing them, you find there are 112 g Brazil nuts, 183 g of cashews, 207 g of almonds, 71 g or hazelnuts, and 446 g of peanuts. You wonder whether your mix is significantly different from what the company advertises? Why is the chi-square goodness-of-fit test NOT appropriate here? 11.14 Example Offspring of certain fruit flies may have yellow or ebony bodies and normal wings or short wings. Genetic theory predicts that these traits will appear in the ratio 9:3:3:1(yellow &amp; normal, yellow &amp; short, ebony &amp; normal, ebony &amp; short) A researcher checks 100 such flies and finds the distribution of traits to be 59, 20, 11, and 10, respectively. What are the expected counts? 56.25, 18.75, 18.75, 6.25 df? 4-1=3 Are the results consistent with the theoretical distribution predicted by the genetic model? 11.15 Assumptions 11.16 Hypothesis Statement \\(H_0: \\text{The results are consitent with the theoretical distribution}\\) \\(H_A:\\text{The results are not consitent with the theoretical distribution}\\) 11.17 Calculation fruit_flys&lt;-c(YN=59, YS=20, EN=11, ES=10) fruit_flys ## YN YS EN ES ## 59 20 11 10 Expected_fruit_flys&lt;-c(9/16, 3/16, 3/16, 1/16) chisqtestGC(fruit_flys, p=Expected_fruit_flys, graph=TRUE) ## Chi-squared test for given probabilities ## ## Observed counts Expected by Null Contr to chisq stat ## YN 59 56.25 0.13 ## YS 20 18.75 0.08 ## EN 11 18.75 3.20 ## ES 10 6.25 2.25 ## ## ## Chi-Square Statistic = 5.6711 ## Degrees of Freedom of the table = 3 ## P-Value = 0.1288 11.18 Conclusion Since p-value&gt;\\(\\alpha\\), I fail to reject \\(H_0\\). There is not sufficient evidence to suggest the results are not consitent with the theoretical distribution. 11.19 \\(\\chi^2\\) test for independence Used with categorical, bivariate data from ONE sample Used to see if the two categorical variables are associated (dependent) or not associated (independent). Assumptions &amp; formula remain the same! Hypothesis Statement - written in words \\[H_0:\\text{Two variables are independent}\\\\ H_A:\\text{Two variables are dependent}\\] Be sure to write in context! 11.20 Example A beef distributor wishes to determine whether there is a relationship between geographic region and cut of meat preferred. If there is no relationship, we will say that beef preference is independent of geographic region. Suppose that, in a random sample of 500 customers, 300 are from the North and 200 from the South. Also, 150 prefer cut A, 275 prefer cut B, and 75 prefer cut C. Based on the information, we can make a table like this: North South Total Cut A 90 60 150 Cut B 165 110 275 Cut C 45 30 75 Total 300 200 500 Find the expected counts. 11.21 Better Way to find the Extected Counts \\[E=\\frac{\\text{row total}\\cdot \\text{column total}}{\\text{grand total}}\\] ## Degrees of freedom \\[df=(r-1)(c-1)\\] Where r is the number rows, c is the number of column. Ignore the row for the total and the column for the total 11.22 Question Suppose the observe vaules for the question above is below: | |North|South|Total| |—|—–|—–|—-| |Cut A| 100 | 50 |150| |Cut B| 150 | 125 |275| |Cut A| 50 | 25 |75| |Total|300|200|500| Is there sufficient evidence to suggest that geographic regions and beef preference are not independent? 11.23 Assumptions SRS All expected \\(\\geq 5\\) 11.24 Hypothesis Statement \\[H_0: \\text{Geographic regions and beef preference are independent}\\\\ H_A: \\text{Geographic regions and beef preference are not independent}\\] 11.25 Calculation # make a contingency table first. Don&#39;t include the totals beef&lt;-matrix(c(100,150,50,50,125,25),3,2, dimnames = list(c(&quot;Cut A&quot;,&quot;Cut B&quot;, &quot;Cut C&quot;), c(&quot;North&quot;, &quot;South&quot;))) beef ## North South ## Cut A 100 50 ## Cut B 150 125 ## Cut C 50 25 # chi-square test chisqtestGC(beef, graph = TRUE) ## Pearson&#39;s Chi-squared test ## ## Observed Counts: ## North South ## Cut A 100 50 ## Cut B 150 125 ## Cut C 50 25 ## ## Counts Expected by Null: ## North South ## Cut A 90 60 ## Cut B 165 110 ## Cut C 45 30 ## ## Contributions to the chi-square statistic: ## North South ## Cut A 1.11 1.67 ## Cut B 1.36 2.05 ## Cut C 0.56 0.83 ## ## ## Chi-Square Statistic = 7.5758 ## Degrees of Freedom of the table = 2 ## P-Value = 0.0226 11.26 Interpretation Since p-value&lt;\\(\\alpha\\), I reject \\(H_0\\). There is sufficient evidence to suggest that geographic regions and beef preference are not independent. 11.27 \\(\\chi^2\\) Test for Homogenity Assumptions &amp; formula remain the same! Expected counts &amp; df are found the same way as test for independence. Only change is the hypotheses! Hypothesis Statement - written in words \\[H_0:\\text{The distributions are the same}\\\\ H_A:\\text{The distributions are different}\\] Be sure to write in context! The following data is on drinking behavior for independently chosen random samples of male and female students. Does there appear to be a gender difference with respect to drinking behavior? (Note: low = 1-7 drinks/wk, moderate = 8-24 drinks/wk, high = 25 or more drinks/wk) Men Women None 140 186 Low 478 661 Moderate 300 173 High 63 16 11.28 Assumptions SRS All expected \\(\\geq 5\\) 11.29 Hypothesis Statement \\[H_0: \\text{Ther is no gender difference with respect to drinking behavior}\\\\ H_A: \\text{There is gender difference with respect to drinking behavior} \\] 11.30 Calculation drinking&lt;-matrix(c(140,478,300,63,186,661, 173, 16),4,2, dimnames = list(c(&quot;None&quot;,&quot;Low&quot;, &quot;Moderate&quot;, &quot;High&quot;), c(&quot;Men&quot;, &quot;Women&quot;))) drinking ## Men Women ## None 140 186 ## Low 478 661 ## Moderate 300 173 ## High 63 16 chisqtestGC(drinking, graph=TRUE) ## Pearson&#39;s Chi-squared test ## ## Observed Counts: ## Men Women ## None 140 186 ## Low 478 661 ## Moderate 300 173 ## High 63 16 ## ## Counts Expected by Null: ## Men Women ## None 158.56 167.44 ## Low 553.97 585.03 ## Moderate 230.05 242.95 ## High 38.42 40.58 ## ## Contributions to the chi-square statistic: ## Men Women ## None 2.17 2.06 ## Low 10.42 9.87 ## Moderate 21.27 20.14 ## High 15.72 14.89 ## ## ## Chi-Square Statistic = 96.5263 ## Degrees of Freedom of the table = 3 ## P-Value = 0 11.31 Conclusion Since p-value&lt;\\(\\alpha\\), I reject \\(H_0\\). There is sufficient evidece to suggest that there is gender difference with respect to drinking behavior. 11.32 Use A Real Dataset Is there any relationship, in the Georgetown College population, between sex and seating preference? xtabs(~sex+seat, data=m111survey) ## seat ## sex 1_front 2_middle 3_back ## female 19 16 5 ## male 8 16 7 # get a barchart barchartGC(~sex+seat, data=m111survey, type=&quot;percent&quot;) # chi-square test chisqtestGC(~sex+seat,data=m111survey, graph=TRUE) ## Pearson&#39;s Chi-squared test ## ## Observed Counts: ## seat ## sex 1_front 2_middle 3_back ## female 19 16 5 ## male 8 16 7 ## ## Counts Expected by Null: ## seat ## sex 1_front 2_middle 3_back ## female 15.21 18.03 6.76 ## male 11.79 13.97 5.24 ## ## Contributions to the chi-square statistic: ## seat ## sex 1_front 2_middle 3_back ## female 0.94 0.23 0.46 ## male 1.22 0.29 0.59 ## ## ## Chi-Square Statistic = 3.734 ## Degrees of Freedom of the table = 2 ## P-Value = 0.1546 "]
]
